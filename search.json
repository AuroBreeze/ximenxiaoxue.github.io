[{"title":"python爬虫系列-JS逆向网易云评论","url":"/2024/07/29/python%E7%88%AC%E8%99%AB%E7%B3%BB%E5%88%97-JS%E9%80%86%E5%90%91%E7%BD%91%E6%98%93%E4%BA%91%E8%AF%84%E8%AE%BA/","content":"python爬虫系列-JS逆向网易云评论python代码下载所需要的库\npip install requestspip install pycryptodome#实现AES加密\n\n\nimport requestsimport jsonfrom Crypto.Cipher import AESfrom Crypto.Util.Padding import padfrom base64 import b64encodeheaders = &#123;    &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&#x27;&#125;data = &#123;        &quot;csrf_token&quot;: &quot;&quot;,        &quot;cursor&quot;: &quot;-1&quot;,        &quot;offset&quot;: &quot;0&quot;,        &quot;orderType&quot;: &quot;1&quot;,        &quot;pageSize&quot;: &quot;20&quot;,        &quot;rid&quot;: &quot;R_SO_4_2061978961&quot;,        &quot;threadId&quot;: &quot;R_SO_4_2061978961&quot;    &#125;f = &quot;00e0b509f6259df8642dbc35662901477df22677ec152b5ff68ace615bb7b725152b3ab17a876aea8a5aa76d2e417629ec4ee341f56135fccf695280104e0312ecbda92557c93870114af6c9d05c4f7f0c3685b7a46bee255932575cce10b424d813cfe4875d3e82047b97ddef52741d546b8e289dc6935b3ece0462db0a22b8e7&quot;e = &#x27;010001&#x27;g = &#x27;0CoJUm6Qyw8W8jud&#x27;i = &quot;ipvDWl7KgbnVS7Ot&quot;url = &quot;https://music.163.com/weapi/comment/resource/comments/get?csrf_token=&quot;def get_encSecKey():    return &quot;d25ce6c769e3acd5a638ef7abdd7c64e2dc3e848f6bfac507cd70ef7f45b16224aebb45a901b4660fe1ad4d36c4916432ef9e11db41c72269d3ae23e0515855a019d962b76c79dfd3b839c2653b7444757915d614ca822b20602ba99b4472a43bb62059ee6afcc241f9be7547ba96e3829651589288743a31548b9324a12dc07&quot;def get_params(data):#默认收到的是字符串    first =enc_params(data,g)    second = enc_params(first,i)    return seconddef enc_params(data, key):    iv = b&#x27;0102030405060708&#x27;  # IV 应该是一个字节序列    key = key.encode(&#x27;utf-8&#x27;)  # 确保 key 是字节序列    aes = AES.new(key=key, mode=AES.MODE_CBC, IV=iv)  # 创建加密器对象    # 填充数据到16字节边界    padded_data = pad(data.encode(&#x27;utf-8&#x27;), AES.block_size)    # 加密填充后的数据    encrypted_data = aes.encrypt(padded_data)    # 返回加密后的数据的 Base64 编码字符串    return b64encode(encrypted_data).decode(&#x27;utf-8&#x27;)data=&#123;    &quot;params&quot;: get_params(json.dumps(data)),#转化为字符串    &quot;encSecKey&quot;:get_encSecKey()&#125;res = requests.post(url, headers=headers, data=data).json()content_list = res[&#x27;data&#x27;][&quot;hotComments&quot;]for content in content_list:    print(content[&#x27;content&#x27;])    print(&quot;--&quot; * 30)\n\n正文准备工作\n首先，我们需要安装一些依赖库，这里我们需要用到requests和pycryptodome两个库。\npip install requestspip install pycryptodome\n\n使用requests进行网络请求，使用pycryptodome进行加密解密。\n\n代码:分析这次我们的目标是获取相关歌曲的评论\n我们这次选择的网址为：https://music.163.com/#/song?id=2061978961\n我们还是和之前一样先去看网站的结构，看看我们要爬取的评论是不是在网页源代码里面\n\n我们发现在这个页面代码里面，可是我们如果去用requests库直接获取的时候，就会发现，我们什么也获取不到，因为网易云的评论是通过AJAX请求获取的。\n\n我们在看到页面源代码后按ctrl+F搜索明年这个刚才我们看到的关键字，但是什么也没有搜索出来，说明评论并不是在源代码里面。\n\n这样我们就只能去翻网易云的网络请求，看看这里有没有评论的相关信息，大家可以自己去每个都翻一下，找找，我们这里就不演示了，直接给大家展示出来。\n\n我们可以看到，我们需要爬取的评论都在这个网络请求里面。\n这时候我们可能就在想，既然在这里面，那我直接找到url，直接获取不就行了吗？\n\n可是，你看这url里面有个csrf_token，我感觉总有一些不对劲。\n\n我们看，我们这样是什么东西都获取不到的。\ncsrf_token是需要参数的，我们需要获取到这个参数才能获取到评论。\n\n也就是说，我们需要获取到params和encSecKey这两个参数才能获取到评论。\n\n代码:JS逆向分析我们当前看到的params和encSecKey是通过JS加密的，我们需要去找哪里的JS代码进行了加密。\n找js我们就去发起程序中找。\n\n\n\n我们直接从这里点进去看，就可以看到js加密的一步一步的过程。\n\n图中加断点的地方就是最后把请求发送过来的JS，我们再看右侧的侧边栏。\n\n在这里，我们要找的params还是被加密的了，我们要先找到没有被加密的堆栈，我们就在红框下面一个一个的往下点，关注右边的侧边栏，直到我们看到params没有被加密。\n\n\n\n\n\n**这个时候，params还是加密的我们继续向下面找，继续点击堆栈的下一个程序。\n\n\n接下来重点就要来了\n\n\n\n\n这两张图片我们可以看到，在tox.be0x这个堆栈下面的匿名程序中，直接就没有了params的数据，而在tox.be0x这个堆栈中，又有了params，可见，params的加密就发生在tox.be0x里面。\n\n\n我们来仔细观察一下侧边栏，再回想一下我们要找的数据是什么?我们的url:”https://music.163.com/weapi/comment/resource/comments/get?csrf_token=&quot;,需要两个参数，一个是`params`,一个是`encSecKey`，我们在侧边栏中就会发现`encSecKey`直接给了，而`params`的数据就是`encText`这个数据。\n现在我们需要干的事情就是，把加密的过程给弄懂，然后咱们把params和encSecKey都手搓出来。\n我们来观察中间的代码。\n\n在这里我们可以观察到params和encSecKey都是在bvi6c返回的json数据中获取的，我们可以在bvi6c上面打一下断电看看，数据的变化情况，在这里我就不展示了。我们可以清楚的看到是bvi6c这个函数进行了加密。\n大家也可以把下面的滑块往右滑动，看看bvi6c的数据，在上面的数据这种并没有关于params和encSecKey信息的变化，而到了bvi6c这个函数中，就出现了我们要找的encText和encSecKey了。\n\n所以，可以大胆推断，加密程序，一定是：\nvar bVi6c = window.asrsea(JSON.stringify(i0x), bse6Y([&quot;流泪&quot;, &quot;强&quot;]), bse6Y(Qu1x.md), bse6Y([&quot;爱心&quot;, &quot;女孩&quot;, &quot;惊恐&quot;, &quot;大笑&quot;]));\n\n我们先看里面的四个参数，首先是JSON.stringify(i0x),在刚才的右边栏中，我们能找到i0x的数据,即：\ncsrf_token: &quot;&quot;,cursor: &quot;-1&quot;,offset: &quot;0&quot;,orderType: &quot;1&quot;,pageSize: &quot;20&quot;,rid: &quot;R_SO_4_2061978961&quot;,threadId: &quot;R_SO_4_2061978961&quot;\n\n在js中，JSON.stringify(i0x)用于将一个 JavaScript 对象或值转换成 JSON 字符串，那很好，我们的第一个参数就知道了。\n第二个参数bse6Y([&quot;流泪&quot;, &quot;强&quot;])，这，我们也看不出什么东西，在源代码中搜索也不会搜出什么东西，这个时候，我们也就需要发挥一下，控制台的作用了。\n\n我们在控制台输入bse6Y([&quot;流泪&quot;, &quot;强&quot;])，然后回车，我们就能看到bse6Y([&quot;流泪&quot;, &quot;强&quot;])的结果了，这也是一个固定的值，剩下的两个参数bse6Y(Qu1x.md)和bse6Y([&quot;爱心&quot;, &quot;女孩&quot;, &quot;惊恐&quot;, &quot;大笑&quot;])也一样，在控制台可以看到他们的参数，也是固定的。\n\n这样，加密用的四个参数，我们就全拿到手了，接下来，我们去看加密的过程。\n\n代码:加密过程分析首先说明，这里设置的a,b,c,d的函数，以及传入的a,b,c,d的参数，大部分都是来混淆你的，函数就是函数，参数就是参数，大家要分清楚。\n在上面的源代码中，我们发现他是使用的window.asrsea这个函数进行的加密，所以，这是个啥函数，正常的js中可没有这个函数，这样，我们就只能去在页面源代码中搜索了，按ctrl+F搜索window.asrsea关键字，我们发现有两处使用了这个关键字，加密的地方算一次，那么第一次看来就是函数定义的地方了。\n\n\n我们可以看到，window.asrsea函数就是d函数，他还想用window.asrsea这个名字混淆咱们。\n\n\n这样我们来分析d函数。\nvar h = &#123;&#125;; //声明一个空对象 hi = a(16);//这行代码调用了一个函数 a 并传入参数 16，生成一个16位的随机字符串return h.encText = b(d, g)//将 b(d, g) 的结果赋值给 h.encTexth.encText = b(h.encText, i) //将 b(h.encText, i) 的结果赋值给 h.encTexth //最后返回对象 h\n\n在这里b函数调用了两次，第二次才是真正的encText。\n这个return语句，在这里并没有运行到这里就退出的作用。\n\n我们来看看a函数。\n  function a(a) &#123;    var d, e, b = &quot;abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789&quot;, c = &quot;&quot;;    for (d = 0; a &gt; d; d += 1)        e = Math.random() * b.length,        e = Math.floor(e),        c += b.charAt(e);    return c&#125;\n\n这个函数就是生成一个16位的随机字符串，我们可以看到，他是用Math.random()生成了一个随机数，然后用Math.floor()取整，然后用charAt()取出字符，然后拼接起来,最后把c返回，这个函数的作用就是生成一个随机字符串。\n所以，i是一个随机的字符串。\n\n按照d函数的逻辑，接下来就是h.encText = b(d, g),这里调用了b函数，我们来看看b函数。\nfunction b(a, b) &#123;    var c = CryptoJS.enc.Utf8.parse(b)      , d = CryptoJS.enc.Utf8.parse(&quot;0102030405060708&quot;)      , e = CryptoJS.enc.Utf8.parse(a)      , f = CryptoJS.AES.encrypt(e, c, &#123;        iv: d,        mode: CryptoJS.mode.CBC    &#125;);    return f.toString()&#125;\n\n\nvar c &#x3D; CryptoJS.enc.Utf8.parse(b)   &#x2F;&#x2F;将第二个参数 b 解码为 UTF-8 格式的字节数组，存储在变量 c 中。\nvar d &#x3D; CryptoJS.enc.Utf8.parse(“0102030405060708”)   &#x2F;&#x2F;解码一个硬编码的16字节的初始化向量（IV），存储在变量 d 中。\nvar e &#x3D; CryptoJS.enc.Utf8.parse(a)   &#x2F;&#x2F;将第一个参数 a 解码为 UTF-8 格式的字节数组，存储在变量 e 中。\nvar f &#x3D; CryptoJS.AES.encrypt(e, c, { … });   &#x2F;&#x2F;使用 AES 加密算法和 CBC 模式加密变量 e，使用变量 c 作为密钥，变量 d 作为 IV。加密结果存储在变量 f 中。\nreturn f.toString()   &#x2F;&#x2F;返回加密结果的字符串表示形式。\n\n\n在这里我们需要知道的是，在这里实现AES的CBC加密算法，需要四个参数\n\n明文（Plaintext）：这是需要加密的原始数据。\n密钥（Key）：加密数据时使用的密钥。对于 AES，密钥长度可以是 128 位（16 字节）、192 位或 256 位。\n初始向量（IV）：一个随机生成的值，用于初始化加密过程。在 CBC 模式中，IV 必须是唯一的，并且长度通常与块大小相同（对于 AES，通常是 16 字节）。\n加密模式（Mode）：AES 加密算法支持三种模式：ECB、CBC 和 CFB。在 CBC 模式中，每个明文块先与上一个密文块进行异或运算后，再与密钥进行加密。在这个例子中，指定为 CBC（密码块链接模式）。CBC 模式是一种常用的加密模式，它使用前一个块的加密结果来加密当前块，从而提高安全性。\n\n\n但是说实话，我们并不需要知道这么多(当然，你如果深入了解这个的话，能更好的帮助你写代码)。\n在这里的代码\nf = CryptoJS.AES.encrypt(e, c, &#123;            iv: d,            mode: CryptoJS.mode.CBC        &#125;)\n\n\ne为加密的明文数据,c为加密的密钥,iv为加密的初始向量，模式选择CBC。\n\nb函数是由d函数调用的，a为传入的b函数的第一个参数，d函数传入b函数的第一个参数就是我们获取到的JSON.stringify(i0x)数据。\nfunction d(d, e, f, g) &#123;        var h = &#123;&#125;          , i = a(16);        return h.encText = b(d, g),        h.encText = b(h.encText, i),        h.encSecKey = c(i, e, f),        h    &#125;    function b(a, b) &#123;        var c = CryptoJS.enc.Utf8.parse(b)          , d = CryptoJS.enc.Utf8.parse(&quot;0102030405060708&quot;)          , e = CryptoJS.enc.Utf8.parse(a)          , f = CryptoJS.AES.encrypt(e, c, &#123;            iv: d,            mode: CryptoJS.mode.CBC        &#125;);        return f.toString()    &#125;\n\n单纯这样讲可能不太直白，大家来看图\n\n这样加密完后，用toString()方法转换成字符串。\n\n按照d函数的逻辑，接下来就是h.encSecKey = c(i, e, f),这里调用了c函数，我们来看看c函数。\nfunction c(a, b, c) &#123;    var d, e;    return setMaxDigits(131),    d = new RSAKeyPair(b,&quot;&quot;,c),    e = encryptedString(d, a)&#125;\n\n但是，如果我们仔细看d函数的调用，我们就会发现一个，有趣的事情。\n在d函数中调用了c函数，并传入了三个值，第一个值是i，第二个值是e，第三个值是f。\n\ni：这个值是随机生成的16位字符串\ne：这个值是bse6Y([&quot;流泪&quot;, &quot;强&quot;])\nf：这个值是bse6Y([&quot;爱心&quot;, &quot;女孩&quot;, &quot;惊恐&quot;, &quot;大笑&quot;])\n\ne和f都是固定的值，那我们把i给固定住，那么c函数不就只返回一个固定的值了吗？大家可以在这个地方打个断点，看看i的值，并复制下来直接用，因为c函数还有其他更多函数的分析，我们就不展开了。\n先在这里打断点\n直到右边栏出现这个，也就是我们要找的url\n\n我们再回到d函数打断点\n\n然后我们一步一步的运行，找到i以及由i生成的encSecKey\n\n把这两个复制下来。\n\n至此，我们已经分析完了加密过程，拿到了我们应该用到的参数，接下来我们就去python，实现代码的复原。\n\n代码:python实现解密过程import requestsimport jsonfrom Crypto.Cipher import AESfrom Crypto.Util.Padding import padfrom base64 import b64encodeheaders = &#123;    &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&#x27;&#125;data = &#123;        &quot;csrf_token&quot;: &quot;&quot;,        &quot;cursor&quot;: &quot;-1&quot;,        &quot;offset&quot;: &quot;0&quot;,        &quot;orderType&quot;: &quot;1&quot;,        &quot;pageSize&quot;: &quot;20&quot;,        &quot;rid&quot;: &quot;R_SO_4_2061978961&quot;,        &quot;threadId&quot;: &quot;R_SO_4_2061978961&quot;    &#125;#bse6Y(Qu1x.md)f = &quot;00e0b509f6259df8642dbc35662901477df22677ec152b5ff68ace615bb7b725152b3ab17a876aea8a5aa76d2e417629ec4ee341f56135fccf695280104e0312ecbda92557c93870114af6c9d05c4f7f0c3685b7a46bee255932575cce10b424d813cfe4875d3e82047b97ddef52741d546b8e289dc6935b3ece0462db0a22b8e7&quot;e = &#x27;010001&#x27;#bse6Y([&quot;流泪&quot;, &quot;强&quot;])g = &#x27;0CoJUm6Qyw8W8jud&#x27; #bse6Y([&quot;爱心&quot;, &quot;女孩&quot;, &quot;惊恐&quot;, &quot;大笑&quot;])i = &quot;dTq5gXUy86kvKfID&quot;url = &quot;https://music.163.com/weapi/comment/resource/comments/get?csrf_token=&quot;def get_encSecKey():#我们的i值已经固定，所以这里直接返回固定值    return &quot;c56d5b52597656e3d4f0efbe1438679842b2ce4a858d24b66e5b6eb388373415843a38e16f79acddc959d81c4cc734a30ff690cb1e1c9ea611254c9302d1efbe78348f8d5fd3b0e17b3ab37e6c95e81ec87ec434b811c00446c06ca52b4102be7eae1b8c637a81c31f5ce876f16880fb4e5e62cc20adb7b854c707d6bcd9c742&quot;\n\n首先导入需要的库，requests用于发送请求，json用于处理json数据，AES用于解密，pad用于填充，b64encode用于编码。\n详细说一下\n\njson库，我们需要用到json.dumps()方法将i0x转换成json字符串。因为加密函数 enc_params 需要一个字节序列作为输入。通过先将数据转换为 JSON 字符串，然后编码成字节序列 (.encode(‘utf-8’))，可以满足加密函数的输入要求，也就是说，在加密的时候必须要放入字符串。\n引入的pad函数，用于填充，因为加密函数要求输入的长度必须是16的整数倍，所以需要对数据进行填充。\nb64encode函数，用于编码，因为加密函数的输入要求是字节序列，所以需要将数据编码成字节序列,加密算法要用两次，在js的d函数中有体现。\nAES库，用于加密。\n\ndef get_params(data):#默认收到的是字符串    first =enc_params(data,g)    second = enc_params(first,i)    return seconddef enc_params(data, key):    iv = b&#x27;0102030405060708&#x27;  # IV 应该是一个字节序列    key = key.encode(&#x27;utf-8&#x27;)  # 确保 key 是字节序列    aes = AES.new(key=key, mode=AES.MODE_CBC, IV=iv)  # 创建加密器对象    # 填充数据到16字节边界    padded_data = pad(data.encode(&#x27;utf-8&#x27;), AES.block_size)    # 加密填充后的数据    encrypted_data = aes.encrypt(padded_data)    # 返回加密后的数据的 Base64 编码字符串    return b64encode(encrypted_data).decode(&#x27;utf-8&#x27;)\n\niv是初始化向量，这个东西在刚才的js源码中\n\n我们直接粘过来，这玩意是偏移量，具体干什么，大家有兴趣的可以去看看。\n有一点我们还需要注意，加密的数据必须是16的整数倍，所以我们需要对数据进行填充。\n例如，如果原始数据的长度是 14 字节，块大小是 16 字节，那么需要添加 2 个字节的填充。\n因为加密后的数据不能直接decode成字符串，所以我们需要先用b64encode编码成字符串，然后再decode成字节序列。\n到这里整个代码就讲解完成了\n\n如果你感觉这篇文章不错，而且财力充足的话，希望可以用爱发电支持一下作者，感谢支持！\n爱发电地址:https://afdian.com/a/AuroraBreeze\n\n","categories":["python"],"tags":["爬虫"]},{"title":"排序算法","url":"/2024/07/09/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/","content":"排序算法在这里我们讲解我们在pywiif中使用的快速排序算法。\n\n快速排序   非常高效，采用分治法。然而，分治法也可能导致大量的递归调用，从而增加调用栈的负担，这在某些情况下可能导致栈溢出。\n\n\n快速排序算法快速排序算法代码：\nimport randomdef quicksort(array):    if len(array) &lt;= 1:        return array    else:        # 随机选择基准元素        pivot_index = random.randint(0, len(array) - 1)        pivot = array[pivot_index]        left = []  # 小于基准的元素        right = []  # 大于基准的元素        mid = []  # 等于基准的元素        for value in array:            if value &lt; pivot:                left.append(value)            elif value &gt; pivot:                right.append(value)            else:                mid.append(value)        # 将较大的元素放在前面        return quicksort(right) + mid + quicksort(left)# 执行快速排序sorted_array = quicksort([3, 6, 8, 10, 1, 2, 1])print(sorted_array)\n\n\n算法的步骤如下：\n\n选择一个基准值，我们通常选择中间的元素作为基准值。\n遍历数组，将小于基准值的元素放到左边，大于基准值的元素放到右边。\n递归地对左右两边的子数组进行相同的操作。\n\n\n如果数组就只有一个元素，就没有必要进行排序，直接返回，用if判断即可。\n当数组的长度大于1时，我们随机选择一个元素作为基准值，用random.randint(0, len(array) - 1)函数随机选择一个索引作为基准值。\n这里我们选择10做为基准值，设立两个放置元素的列表left和right，然后遍历数组，将小于10的元素放到left，大于10的元素放到right。\n这样我们的left列表里就只有小于10的元素，right列表里就只有大于10的元素。即left列表是[3，6，8，1，2，1]，right列表为空。\n到这个函数的最后，return quicksort(right) + mid + quicksort(left)会再次调用quicksort函数，对right列表和left进行排序，步骤和前面一样。\n最后，我们将排好的right列表、mid列表和left列表的元素合并，得到一个排序好的数组。\n","categories":["算法"],"tags":["算法"]},{"title":"python爬虫系列:beautifulsoup爬虫与线程池","url":"/2024/06/21/python%E7%88%AC%E8%99%AB%E7%B3%BB%E5%88%97-beautifulsoup%E7%88%AC%E8%99%AB%E4%B8%8E%E7%BA%BF%E7%A8%8B%E6%B1%A0/","content":"python爬虫系列:beautifulsoup爬虫与线程池前言这篇文章，也是对CSDN文章的修改，使用beautifulsoup爬取网页，并使用线程池提高爬取速度。\nPython代码import refrom concurrent.futures import ThreadPoolExecutor #导入线程池，加速爬取import requestsfrom bs4 import BeautifulSoupimport osif not os.path.exists(&quot;webspider_book&quot;):    os.mkdir(&quot;webspider_book&quot;)url =&quot; https://www.bigee.cc/book/78647/&quot;headers = &#123;    &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&quot;&#125;def get_url(url):#要用两次直接写个函数算了    res = requests.get(url,headers=headers)    soup = BeautifulSoup(res.text,&quot;lxml&quot;)    return soupdef get_max_page(url): #获取下载的最大页数    list = get_url(url).select(&quot;body &gt; div.listmain &gt; dl &gt; dd a&quot;)    page_finall = re.findall(r&quot;/book/78647/(.*).html&quot;, str(list[-1][&#x27;href&#x27;]))[0] #正则匹配最后一个章节的页码    #print(page_finall)    return page_finall #返回最大页数def get_book_content(url):#下载单个页面的函数    soup = get_url(url)    name = soup.select(&quot;#read &gt; div.book.reader &gt; div.content &gt; h1&quot;)[0].get_text()    #print(name)    content = soup.select(&quot;#chaptercontent&quot;) #获取章节内容    content = str(content[0].get_text()) #获取章节内容    #print(content)    f = open(f&quot;webspider_book/&#123;name&#125;.txt&quot;, &quot;w&quot;, encoding=&quot;utf-8&quot;)    f.write(content)#写入文件    print(f&quot;&#123;name&#125;下载完成----------&quot;)if __name__ == &#x27;__main__&#x27;:    num = int(get_max_page(url=url))    print(f&quot;共&#123;num&#125;页----------&quot;)    with ThreadPoolExecutor(100) as t: #线程池大小为100        for i in range(1,num+1):            t.submit(get_book_content,f&quot;https://www.bigee.cc/book/78647/&#123;i&#125;.html&quot;) #提交任务    print(&quot;线程池执行完毕&quot;)\n\n正文准备工作\n我们需要下载的是requests和beautifulsoup4库，如果没有安装，请使用以下命令安装,lxml也需要下载。\npip install requests\n\npip install beautifulsoup4\n\npip install lxml\n\n\n对于线程池，我们需要导入concurrent.futures库，并创建一个ThreadPoolExecutor对象，这个就不需要下载了，这是自带的，对于concurren.futures的使用，我们这里只讲解一下ThreadPoolExecutor的使用，其他的内容大家自行探索。\n\n代码:分析思路分析\n这次我们的目标是爬取目标网站的小说，下载下来并用线程池加速下载,目标网址为：\nhttps://www.bigee.cc/book/78647/\n好了，想要爬去内容，我们需要先分析网站的结构，去看看我们要爬取的内容在哪里。\n\n按 F12 进入开发者工具\n\n\n\n这样我们就可以看到了小说的地址到底是不是，我们可以进去看一看。\n\n我们也知道，我们看到的是相对的位置，然后我们在小说内容的部分看到了他的地址\n\nhttps://www.bigee.cc/book/78647/1.html\n\n我们看到的相对位置\n\n&#x2F;book&#x2F;78647&#x2F;1.html\n\n我们这样就察觉到他们之间不就只差了一个https://www.bigee.cc吗？到时候我们只需要拼接一下，就可以拿到我们想要的页面的url了。\n我们再看其他章节的地址，我们可以看到，他们的地址都是类似的，只是最后的数字不同。\n\nhttps://www.bigee.cc/book/78647/2.html\n\n\nhttps://www.bigee.cc/book/78647/3.html\n\n所以如果我们想要下载所有的章节，我们只需要遍历所有的数字，然后拼接一下就可以了。\n那我们就需要先获取到这个最大的页码，然后遍历所有的数字，再在页面中下载内容就可以了。\n代码:获取最大页码那我们现在要做的就是获取最大的页码，我们可以用requests和beautifulsoup4来获取。\nimport requestsfrom bs4 import BeautifulSoupimport reurl =&quot; https://www.bigee.cc/book/78647/&quot;headers = &#123;    &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&quot;&#125; #设置请求头,模拟浏览器res = requests.get(url,headers=headers).text#获取网页内容soup = BeautifulSoup(res,&quot;lxml&quot;)#解析网页内容,加载lxml解析器list = soup.select(&quot;body &gt; div.listmain &gt; dl &gt; dd a&quot;)page_finall = re.findall(r&quot;/book/78647/(.*).html&quot;,list[-1][&#x27;href&#x27;])print(page_finall)\n\n这里我们使用beautifulsoup的select来获取到所有的章节的链接，也就是拿到所以我们匹配的a标签，这会生成一个列表。\n对于BS4怎么去选择这个标签，我们也可以直接去浏览器进行复制，具体BS4的选择方法，大家可以自行探索。\n下面是复制的步骤\n\n\n但是我们复制的结果是：body &gt; div.listmain &gt; dl &gt; dd:nth-child(2) &gt; a\n因为我们要拿到最后一个章节的链接，而这个选择只能让我们拿到第一个章节的url，所以我们需要改一下，改为：body &gt; div.listmain &gt; dl &gt; dd a\n我们就能拿到这样的信息了。\n\n这是一个列表，最后的章节在列表的最后一个，我们可以用list[-1]来获取到最后一个章节的链接。\n我们通过list[-1][&#39;href&#39;]来获取到最后一个章节的链接,这是bs4的一种方法。\n接下来我们用正则表达式来匹配这个链接，正则表达式的匹配规则是：r&quot;/book/78647/(.*).html&quot;\n\n其中（.*）的意思是匹配任意字符，当然必须在/book/78647/和.html之间。\n\n对于正则的使用可以来这里学习：https://deerchao.cn/tutorials/regex/regex.htm\n这样最后我们就拿到了最大的页码了，也就是313\n\n\n\n在这个网站中还有点小问题他隐藏了一部分章节的url在另一个代码块中，大家可以去看看\n\n\n代码:下载我们现在需要先写一个代码把第一页的内容都下载下来，其他的页面，除了url不一样以外其他的相同。\n我们先进入第一页，F12进入开发者工具，分析一下。\n\nimport requestsfrom bs4 import BeautifulSoupimport osif not os.path.exists(&quot;book&quot;):    os.mkdir(&quot;book&quot;)headers = &#123;    &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&quot;&#125;url = &quot;https://www.bigee.cc/book/78647/1.html&quot;res = requests.get(url,headers=headers).textsoup = BeautifulSoup(res,&quot;lxml&quot;)name = soup.select(&quot;#read &gt; div.book.reader &gt; div.content &gt; h1&quot;)[0].get_text() #获取小说名#print(name)content = soup.select(&quot;#chaptercontent&quot;)#获取章节内容content = str(content[0].get_text())#获取章节内容print(content)f = open(f&quot;webspider_book/&#123;name&#125;.txt&quot;,&quot;w&quot;,encoding=&quot;utf-8&quot;)f.write(content)\n\nbs4中获取标签内容的方法，是使用get_text() 方法，我们可以直接获取到章节的内容，但是我们用的select方法，返回的是一个列表，我们就需要拿到列表中的内容，当然，列表中也就只有这一个元素。因为我们选择器选择的#chaptercontent标签（浏览器复制的地址就是这个），所以我们直接用content[0]就能拿到章节的标签，然后用get_text()方法来获取到章节的内容。\n\n小说名同理，直接去网页上用浏览器直接复制地址就可以了。\n然后我们把章节的内容写入文件，文件名就是小说名，我们用f = open(f&quot;webspider_book/&#123;name&#125;.txt&quot;,&quot;w&quot;,encoding=&quot;utf-8&quot;)来创建文件，然后用f.write(content)来写入内容。\n这样我们就下载了第一页的内容。\n\n代码:优化实现我们现在已经有了下载单个页面的函数，我们可以用线程池来加速下载，我们可以用ThreadPoolExecutor来创建线程池，然后用submit()方法来提交任务。\n简单来说，我们不需要用循环来慢慢下载网页内容了，我们直接用线程池来下载，线程池创建本来就需要循环，我们把有规律的url遍历提交给线程池。\n正常放前面不经常用和经常用但不需要改动的数据\nimport refrom concurrent.futures import ThreadPoolExecutor #导入线程池，加速爬取import requestsfrom bs4 import BeautifulSoupimport osif not os.path.exists(&quot;webspider_book&quot;):    os.mkdir(&quot;webspider_book&quot;)url =&quot; https://www.bigee.cc/book/78647/&quot;headers = &#123;    &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&quot;&#125;\n\n在前面的讲述中\nres = requests.get(url,headers=headers)soup = BeautifulSoup(res.text,&quot;lxml&quot;)\n\n用到了两次，所以就干脆写了个函数，这样就不用重复写了，让代码看起来干净些。\ndef get_url(url):#要用两次直接写个函数算了    res = requests.get(url,headers=headers)    soup = BeautifulSoup(res.text,&quot;lxml&quot;)    return soup\n\n因为我们需要实例化，所以这个函数就得把实例化的对象soup返回，这样才可以使用。\ndef get_max_page(url): #获取下载的最大页数    list = get_url(url).select(&quot;body &gt; div.listmain &gt; dl &gt; dd a&quot;)    page_finall = re.findall(r&quot;/book/78647/(.*).html&quot;, str(list[-1][&#x27;href&#x27;]))[0] #正则匹配最后一个章节的页码    #print(page_finall)    return page_finall #返回最大页数\n\n这个函数就是获取最大的页码,但是他也需要先实例化soup对象，所以我们就把get_url()函数也写进来，让他实例化并返回soup对象，最后正则获取到最后一个章节的url。\ndef get_book_content(url):#下载单个页面的函数    soup = get_url(url)    name = soup.select(&quot;#read &gt; div.book.reader &gt; div.content &gt; h1&quot;)[0].get_text()    #print(name)    content = soup.select(&quot;#chaptercontent&quot;) #获取章节内容    content = str(content[0].get_text()) #获取章节内容    #print(content)    f = open(f&quot;webspider_book/&#123;name&#125;.txt&quot;, &quot;w&quot;, encoding=&quot;utf-8&quot;)    f.write(content)#写入文件    print(f&quot;&#123;name&#125;下载完成----------&quot;)\n\n这个函数是下载对应页面的内容，他也需要先实例化soup对象，所以我们继续用我们刚开始写的get_url()函数来实例化，然后用select()方法来获取到小说名，然后用get_text()方法来获取到小说名和章节内容。\nif __name__ == &#x27;__main__&#x27;:    num = int(get_max_page(url=url))    print(f&quot;共&#123;num&#125;页----------&quot;)    with ThreadPoolExecutor(100) as t: #线程池大小为100        for i in range(1,num+1):            t.submit(get_book_content,f&quot;https://www.bigee.cc/book/78647/&#123;i&#125;.html&quot;) #提交任务    print(&quot;线程池执行完毕&quot;)\n\n在这里我们先运行get_max_page()函数来获取最大的页码，url我们在上面已经先声明了，然后我们用ThreadPoolExecutor来创建线程池，线程池大小为100 **(可以自己调整)**，然后用for循环来遍历所有的页码，然后用submit()方法来提交任务，任务就是get_book_content()函数，我们把页码和url拼接起来，然后提交给线程池,这样我们就能很快的把小说爬完了。\n\n如果你感觉这篇文章不错，而且财力充足的话，希望可以用爱发电支持一下作者，感谢支持！\n爱发电地址:https://afdian.com/a/AuroraBreeze\n\n","categories":["python"],"tags":["爬虫"]},{"title":"kaze主题配置建议","url":"/2024/06/21/kaze%E9%85%8D%E7%BD%AE%E5%BB%BA%E8%AE%AE/","content":"kaze主题配置建议\n因为kaze的主题配置文档已经看不了了，而且kaze的github也已经一年多没有进行提交了，并且kaze的主题用的人并不多，所以我就把我摸索的知识来分享一下。\n\n前言请先安装好hexo及kaze主题。\n修改为中文页面kaze主题默认为英文，所以需要修改为中文，但是修改为中文的配置文件并不在kaze主题的-config.yml文件中，而是在hexo的_config.yml文件中。\n所以我们去hexo的_config.yml文件中找到language字段，将其修改为zh-CN。\n\n\n修改完成后，我们的主题页面就变成了中文了。\n\n\n删除不必要的页面kaze主题有很多页面，其中有些页面似乎并没有布置好，我们进去后发现这些页面都是空白的，所以我们可以将这些页面删除掉。\n在这里我们只需要把那些没有用的页面进行注释掉就可以了。\n在kaze的_config.yml中注释掉about页面的配置，可以参照我的配置。\n\n当然要告诉你的一点是，友链的link并没有中文，所以这里我们手动把他改为中文。\n\nkaze主题的_config.yml配置下面大家可以根据图片进行对应配置。\n\n\n\n\n\n\n这里图片上传的方式是图床，大家可以去网上找些免费的图床。\n不过，我还是推荐用自己的文件夹放图片，一是不用担心钱的问题，二是可以自己管理图片，不用担心博客文章里的图片丢失。\n这是我配置图片的信息，给大家看看。\n\n\n我直接在hexo的source文件夹中创建了这两个放图片的文件夹。\n不过，我这种方式有点烂，我还是比较推荐在source文件中只建一个文件夹专门放图片，然后再在创建的文件夹中再创建各个博客文章要用的文件夹，这样方便管理。\n这是我在markdown语言中对图片的引用\n\n用..表示上一级目录，这样再进入创建的文件夹里拿图片就可以了。\n\n\n\n这个链接是有官方的配置的，图标只能用官方的，而且官方的的图标并不多，大家可以去这个网站来看看：\nhttps://www.cnblogs.com/yuanlinblog/p/16654439.html\n\n\n\n\n这个就是友链，我们可以把他的名字什么的改了，avatar是头像，这里要放头像的链接，description就是描述了，当然这里你可以放许多的人。\n\n\n\n注意：kaze的搜索默认关闭，所以我们要把enable改为true才会开启搜索功能。其他的地方就不需要改动了。\n\n\n \n注意：kaze的访问统计是不可用的，开启了也是统计错误的访问次数，所以我们关闭，其他的看注释就好了。\n\n\nkaze主题的评论是默认关闭的，我们把enable改为true就开启了评论功能。\n评论用的插件是valine，这需要我们注册一个leancloud账号，然后在valine的_config.yml文件中填入自己的app_id和app_key。\n大家要去：https://console.leancloud.cn/ 注册一个leancloud账号，然后创建一个应用\n\n\n\n\n然后我们在valine的_config.yml文件中填入自己的app_id和app_key，这样我们就可以用评论功能了。\n具体使用可以去valine的官方文档看看：https://valine.js.org/quickstart.html\n\n这三个箭头分别是数学公式的渲染（默认关闭），代码块的渲染，以及文章字数统计（默认开启）文章的统计分析（默认关闭）。\n\n\n这里改字体大小\n\n博客文章的配置\ntitle：文章标题\ndate：文章日期\ntags：文章标签\ncategories：文章分类\n对应的也就是\n\n\n我们的博客的文章底部也需要修改\n\n不修改的话，文章链接是错误的。\n\n注意：这里文章的链接修改不在kaze的配置文件_config.yml中，而是在hexo的_config.yml文件中,所以我们去hexo的配置文件中修改。\n\n\n在这里把自己的博客地址放上就可以了。\n\n如果你感觉这篇文章不错，而且财力充足的话，希望可以用爱发电支持一下作者，感谢支持！\n爱发电地址:https://afdian.com/a/AuroraBreeze\n\n","categories":["hexo"],"tags":["kaze"]},{"title":"hexo部分问题解决","url":"/2024/06/21/hexo%E9%83%A8%E5%88%86%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/","content":"hexo部分问题解决1. 处理hexo的代码高亮问题在处理kaze主题的时候，因为需要找到一个合适的代码高亮的主题，所以我就在kaze的代码高亮中修改配置文件，同时又修改了hexo的配置文件，但我发现了一个问题，那就是代码高亮的颜色不太对，而且不能及时的修改，不管用hexo g生成几次都是原来的样子。\n经过一番搜索，终于找到了解决办法，下面是解决方法：\n解决方法\n找到hexo的配置文件_config.yml，找到highlight部分，修改hljs为true，把wrap改为true这样就能把我们的代码圈起来,注意的是highlight的显示代码行数在kaze主题中是不能使用的。\n\n用hexo clean命令清除缓存，然后用hexo g命令重新生成静态文件。这一步非常的重要，如果不去清楚缓存，就不会生效。\n\n重新打开博客，代码高亮应该就正常了。\n\n\n\n以上就是解决hexo代码高亮问题的方法，希望能帮到大家。\n\n","categories":["hexo"],"tags":["hexo"]},{"title":"python爬虫系列:Xpath爬取图片","url":"/2024/06/20/python%E7%88%AC%E8%99%AB%E7%B3%BB%E5%88%97-Xpath%E7%88%AC%E5%8F%96%E5%9B%BE%E7%89%87/","content":"python爬虫系列:Xpath爬取图片前言这篇文章很久之前就写完了，可是听说这篇文章有许多的问题，但是我在CSDN上并不敢改这篇文章，因为很可能改了就再也发不出来了，所以趁着我搭建了一个博客，把这篇文章重新写一遍，顺便把之前写的烂代码改一下，希望这篇文章能帮到大家。\n其次，希望大家学习一下基本的html语法。\npython代码import timefrom lxml import etreeimport requestsimport osheaders = &#123;    &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36 Edg/113.0.1774.42&#x27;&#125;url_list = []page = 0if not os.path.exists(&quot;./pic/&quot;):     os.makedirs(&quot;./pic/&quot;)def pic_path():    global page,url_list    while True:        page = input(&quot;输入下载页数：&quot;)        try:            page = int(page)        except ValueError:            print(&quot;输入页数有误！&quot;)            continue        if page == 1:            url = &quot;https://pic.netbian.com/4kdongman/index.html&quot;            return url        elif page &gt; 1:            for i in range(2, page + 1):                url_list.append(&quot;https://pic.netbian.com/4kdongman/index.html&quot;)                url_list.append(&quot;https://pic.netbian.com/4kdongman/index_&#123;&#125;.html&quot;.format(i))            return None        else:            print(&quot;输入页数有误！&quot;)            continuedef download(url):    if url is not None:        response = requests.get(url, headers=headers).text.encode(&#x27;iso-8859-1&#x27;)        Batch_processing(response)    else:        for url in url_list:            response = requests.get(url, headers=headers).text.encode(&#x27;iso-8859-1&#x27;)            Batch_processing(response)def Batch_processing(response):    tree = etree.HTML(response)    pic_list = tree.xpath(&#x27;//*[@id=&quot;main&quot;]/div[3]/ul/li&#x27;)    for pic_list_item in pic_list:        # print(pic_list_item.xpath(&#x27;a/img/@src&#x27;)[0])        # print(pic_list_item.xpath(&#x27;a/b/text()&#x27;)[0])        html_url = &quot;https://pic.netbian.com&quot; + pic_list_item.xpath(&#x27;a/img/@src&#x27;)[0]        name_pat = pic_list_item.xpath(&#x27;a/b/text()&#x27;)[0]        print(html_url)        print(name_pat)        try:            name = str(name_pat.replace(&quot; &quot;, &quot;_&quot;) + &quot;.jpg&quot;)            name = str(name.replace(&quot;*&quot;, &quot;&quot;))  # 处理文件名中含有*的情况，有些文件名中含有*会导致文件无法保存            pic_get = requests.get(html_url, headers=headers).content            pic_save_path = &quot;./pic/&quot; + name            with open(pic_save_path, &#x27;wb&#x27;) as f:                f.write(pic_get)                print(name + &quot;下载完成！&quot;)        except :            print(name + &quot;下载失败！&quot;)        time.sleep(0.2)#print(response)&quot;&quot;&quot;pic_path = tree.xpath(&#x27;//*[@id=&quot;main&quot;]/div[3]/ul/li[1]/a/img&#x27;)pic_name = tree.xpath(&#x27;//*[@id=&quot;main&quot;]/div[3]/ul/li[1]/a/b&#x27;)print(pic_path[0].get(&#x27;src&#x27;))print(pic_name[0].text)&quot;&quot;&quot;if __name__ == &#x27;__main__&#x27;:    url = pic_path()    download(url)\n\n\n\n\n正文准备工作\n首先，我们需要安装lxml库，这是Python的一个第三方库，可以用来解析和操作XML和HTML文档，也是我们使用Xpath爬取网页图片所必须的Python库。\npip install lxml\n\n\n其次是安装requests库，这是Python的一个第三方库，可以用来发送HTTP请求，我们使用它来获取网页内容，基本上大部分爬虫都需要这个库。\npip install requests\n\n\n最后，我们也会用到Python的os模块，用来处理图片的下载，以及time模块，来减缓我们的请求频率，毕竟我们只是练习，我们并不希望用这个爬虫给别人网站造成压力。\n\n代码解析之一思路分析\n这次我们的目标是在这个网站进行图片的爬取下载,网站的网址为\nhttps://pic.netbian.com/4kdongman/index.html\n我们需要分析这个网站的结构，拿到我们所需要的信息，从而进行下载图片。\n我们首先去分析目标网站，我们这次挑选的网站结构比较简单。\n\n按 F12 进入开发者工具\n\n\n通过图片上的步骤我们进入到网站html界面放置图片的位置。\n\n接下来我们去找图片到底在哪里，我们这里只找到了相对位置\n\n\n\n记得刷新，不刷新找不到这些图片\n\n\n我们从这找到了图片的位置，到底是不是这张图片，我门去这个url里看看\n\n我们可以看到，我们找到的url是正确的，所以我们想要下载图片，就必须拿到图片的url。\n\n接下来我们分析我们拿到的相对位置与图片的位置有什么区别？\n\n我们先来看我们拿到的相对位置\n&#x2F;uploads&#x2F;allimg&#x2F;240618&#x2F;202141-1718713301deda.jpg\n我们再来看我们找到的图片的位置\nhttps://pic.netbian.com/uploads/allimg/240618/202141-1718713301deda.jpg\n\n这两个地址之间就差了一个https://pic.netbian.com所以如果我们拿到了相对位置，只需要在相对位置上加上https://pic.netbian.com就能找到图片的url。\n\n\n代码解析之二我们现在先编写代码把相对位置的图片url转换成绝对位置的图片url\n这里的代码与最后的代码不同，我们以后会改。\nimport timefrom lxml import etreeimport requestsimport osheaders = &#123;    &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36 Edg/113.0.1774.42&#x27;&#125;#UA伪装，防止服务器拒绝访问url = &quot;https://pic.netbian.com/4kdongman/index.html&quot;response = requests.get(url, headers=headers).text.encode(&#x27;iso-8859-1&#x27;)#获取网页内容，同时将网页内容编码为iso-8859-1，也就是解析为中文，要不然全是乱码，根本不是给人看到tree = etree.HTML(response)#解析网页内容,用这个来加载xpathpic_path = tree.xpath(&#x27;//*[@id=&quot;main&quot;]/div[3]/ul/li[1]/a/img/@src&#x27;)[0]#获取相对位置的图片urlprint(pic_path)#打印相对位置的图片url\n\n这里我们先看一下找到的是不是相对位置，下面图片是输出\n\n确实是我们想要的输出。\n对于xpath怎么获取图片的url，在浏览器我们可以直接右键获取xpath地址，如有需要还是建议大家去看看xpath的语法。\n以后我们再细讲一下，我们先来分析我们的代码\n\n\n\n两个中随便挑一个就行，我们这里用的是//*[@id=&quot;main&quot;]/div[3]/ul/li[1]/a/img这样为了我们能够拿到html中的图片url，我们需要用到@src属性，这个属性可以帮助我们拿到图片的url，当然返回的是一个列表，我们再用[0]来获取第一个url，就可以拿到图片的url了。//*[@id=&quot;main&quot;]/div[3]/ul/li[1]/a/img/@src\n这样我们就拿到了第一个图片的位置，但是我们需要拿到所以的图片的url，那我们要怎么办呢？\n聪明的同志已经知道了，那就写个循环呗，把所有的url都拿到。\n\n但提醒一点的是，我们是进入到了&lt;li&gt;标签中拿到的图片的URL，我们再分析一下图片的位置，我们会发现，所有的图片都包裹在&lt;li&gt;中，所以我们只需要循环遍历所有的&lt;li&gt;标签，然后再用xpath来获取图片的url就行了。\n\n所以事不宜迟，我们改代码把所有图片的url都拿到。\ntree = etree.HTML(response)#解析网页内容,用这个来加载xpathpic_path = tree.xpath(&#x27;//*[@id=&quot;main&quot;]/div[3]/ul/li&#x27;)#获取相对位置的图片urlfor pic in pic_path:    html_url = &quot;https://pic.netbian.com&quot; + pic.xpath(&#x27;a/img/@src&#x27;)[0]    print(html_url)\n\n\n\ntree = etree.HTML(response)#解析网页内容,用这个来加载xpath这个代码及以上都没有改动，只有他的下面进行了改动，以后我们也是这样提示我们所改动的地方!!!!!!。\n\n\n下面是输出,这样我们就拿到了所有图片的url了，当然只是这一页，下面的页数我们稍后再讲。\n\n我们再来获取他的名字，这个我们就不解释了，希望大家自己尝试一下，我们直接给出代码。\npic_path = tree.xpath(&#x27;//*[@id=&quot;main&quot;]/div[3]/ul/li&#x27;)#获取相对位置的图片urlfor pic in pic_path:    html_url = &quot;https://pic.netbian.com&quot; + pic.xpath(&#x27;a/img/@src&#x27;)[0]    name_url = pic.xpath(&#x27;a/b/text()&#x27;)[0]    print(html_url)    print(name_url)\n下面是输出\n\n代码解析之三我们现在开始编写代码来下载图片\n既然名字和地址我们都拿到了，剩下的就是怎么把图片给下载下来了。\n\n这里有一点要注意，图片保存的时候名字有限制，比如我们下载的第一个图片的名字中有星号，这就不行，所以我们需要对名字进行处理，比如把空格替换成下划线，把*替换成其他字符等。\n\nfor pic in pic_path:    html_url = &quot;https://pic.netbian.com&quot; + pic.xpath(&#x27;a/img/@src&#x27;)[0]    name_url = pic.xpath(&#x27;a/b/text()&#x27;)[0]    print(html_url)    print(name_url)    name = str(name_url.replace(&quot; &quot;, &quot;_&quot;) + &quot;.jpg&quot;)    name = str(name.replace(&quot;*&quot;, &quot;&quot;))  # 处理文件名中含有*的情况，有些文件名中含有*会导致文件无法保存        pic_get = requests.get(html_url, headers=headers).content#获取图片内容            pic_save_path = &quot;./pic/&quot; + name # 图片保存路径    if not os.path.exists(&quot;./pic/&quot;):#判断图片保存路径是否存在，不存在则创建        os.makedirs(&quot;./pic/&quot;)    with open(pic_save_path, &#x27;wb&#x27;) as f:        f.write(pic_get)        print(name + &quot;下载完成！&quot;)                time.sleep(0.3) # 防止被服务器封IP\n下面是输出\n\n首先，我们获取的名字需要处理，我们需要把名字中的空格和星号都处理掉，这样才能保存为一个文件名，同时我们要加入.jpg后缀，才能保存图片，我们就用replace()函数来替换空格和星号，然后加上.jpg后缀，replace函数的第一个参数是被替换的字符，第二个参数是替换成什么字符。\n然后，我们获取图片的url，然后用requests库来获取图片的内容，requests.get(url, headers=headers).content,这里的content是持续化存储，我们要用这种方式存储图片。\n我们再用os库来判断图片保存路径是否存在，不存在则创建，然后用with open(pic_save_path, &#39;wb&#39;) as f:来写入图片内容，最后打印下载完成。\n\n对于with open(pic_save_path, &#39;wb&#39;) as f:这句代码，我们用wb模式打开文件，wb模式是写入二进制文件，with open(pic_save_path, &#39;wb&#39;) as f:的意思是打开一个文件，文件名为pic_save_path，以二进制写入模式打开，然后用f.write(pic_get)来写入图片内容，pic_get是获取到的图片内容。\n\n最后，我们用time.sleep(0.3)来防止被服务器封IP，毕竟我们只是练习，我们并不希望用这个爬虫给别人网站造成压力。\n代码解析之四我们再来下载其他页面的图片\n说实话到这里基本上已经完成一半多了，我们再坚持一下，就成功了。\n我们先来分析一下其他页面的图片的位置。\n第一页\n\n第二页\n\n第三页\n\n\n我们可以看到，第一页为index.html，第二页为index_2.html，第三页为index_3.html。\n\n大家可以自己分析一下第二，三页图片的位置，是和第一页一样的，我们不再赘述。\n这样我们就得开动一下脑筋，第一页和其他页的url有差别，但是第二三四及以后的页面的url是有规律的，我们可以用循环来实现，但是第一页呢？\n当然我们可以直接用笨办法，写两个循环，但感觉太臃肿了，不太好。\n代码解析之五实现思路及优化\n\n思路导图\n\n\nimport timefrom lxml import etreeimport requestsimport osheaders = &#123;    &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36 Edg/113.0.1774.42&#x27;&#125;if not os.path.exists(&quot;./pic/&quot;):    os.makedirs(&quot;./pic/&quot;)url_list = []page = 0def pic_path():    global page,url_list    while True:        page = input(&quot;输入下载页数：&quot;)        try:            page = int(page)        except ValueError:            print(&quot;输入页数有误！&quot;)            continue        if page == 1:            url = &quot;https://pic.netbian.com/4kdongman/index.html&quot;            return url        elif page &gt; 1:            for i in range(2, page + 1):                url_list.append(&quot;https://pic.netbian.com/4kdongman/index.html&quot;)                url_list.append(&quot;https://pic.netbian.com/4kdongman/index_&#123;&#125;.html&quot;.format(i))            return None        else:            print(&quot;输入页数有误！&quot;)            continuedef download(url):    if url is not None:        response = requests.get(url, headers=headers).text.encode(&#x27;iso-8859-1&#x27;)        Batch_processing(response)    else:        for url in url_list:            response = requests.get(url, headers=headers).text.encode(&#x27;iso-8859-1&#x27;)            Batch_processing(response)def Batch_processing(response):    tree = etree.HTML(response)    pic_list = tree.xpath(&#x27;//*[@id=&quot;main&quot;]/div[3]/ul/li&#x27;)    for pic_list_item in pic_list:        html_url = &quot;https://pic.netbian.com&quot; + pic_list_item.xpath(&#x27;a/img/@src&#x27;)[0]        name_pat = pic_list_item.xpath(&#x27;a/b/text()&#x27;)[0]        print(html_url)        print(name_pat)        try:            name = str(name_pat.replace(&quot; &quot;, &quot;_&quot;) + &quot;.jpg&quot;)            name = str(name.replace(&quot;*&quot;, &quot;&quot;))  # 处理文件名中含有*的情况，有些文件名中含有*会导致文件无法保存            pic_get = requests.get(html_url, headers=headers).content            pic_save_path = &quot;./pic/&quot; + name            with open(pic_save_path, &#x27;wb&#x27;) as f:                f.write(pic_get)                print(name + &quot;下载完成！&quot;)        except :            print(name + &quot;下载失败！&quot;)        time.sleep(0.2)if __name__ == &#x27;__main__&#x27;:    url = pic_path()    download(url)\n\n我的思路是首先把一些无用的资源放到外面，减少资源的浪费，比如说headers、url_list、page，比较headers用的话只用他的常量，放到循环里面，不断的加载他却不用，就是资源的浪费。\n\n\n毕竟第一页和其他页并没有什么规律，我就打算把其他页的url都放到url_list里面。\n\n\n对于page在外面就是随便设置的一个量，你设置1000000000000000也没有问题，我们主要是用page判断你要下载的页面数量，肯定是要立马获取的，一进入程序就会把page的值进行修改，所以我觉得还是放到外面比较好。\n\n首先，我们定义了一个函数pic_path(),在这个函数里面，我们先把page和url_list都放到全局变量里面,因为我们要在函数中改变page的值，所以用全局变量，让其他的函数可以访问到。\n之后，我们用input获取我们要下载的页数，然后用try except来判断输入的是否为数字，如果不是数字，就提示输入有误，如果是数字，就判断是否大于1，如果大于1，就把url_list里面添加url(毕竟第一页和其他页没有联系，所以我们就直接把直接把第一页的url放到url_list里面)，如果等于1，就返回url。\n所以当我们要下载的页数超过两页的时候，我们就让函数遍历url_list，获取url。\n然后，我们定义了一个函数download(url),这个函数就是用来下载图片的，我们先判断url是否为None，如果不是None，就说明是第一页，我们就直接调用Batch_processing(response)函数，如果是None，就说明不是第一页，我们就遍历url_list，调用Batch_processing(response)函数。\n然后，我们定义了一个函数Batch_processing(response),这个函数就是用来处理图片的，我们先用etree来解析网页内容，然后用xpath来获取图片的url和名字，然后用requests来获取图片的内容，然后用os来判断图片保存路径是否存在，不存在则创建，然后用with open(pic_save_path, &#39;wb&#39;) as f:来写入图片内容，最后打印下载完成。\n最后，我们用if __name__ == &#39;__main__&#39;:来判断是否是主函数，如果是，就调用pic_path()函数，获取url，然后调用download(url)函数，开始下载图片。\n这样，我们就实现了下载所有页面的图片。\n\n如果你感觉这篇文章不错，而且财力充足的话，希望可以用爱发电支持一下作者，感谢支持！\n爱发电地址:https://afdian.com/a/AuroraBreeze\n\n","categories":["python"],"tags":["爬虫"]}]