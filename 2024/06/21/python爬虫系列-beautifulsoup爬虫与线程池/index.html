<!DOCTYPE html>
<html lang="zh-CN">
  <head>
  <meta charset="UTF-8">
  <meta 
    name="viewport"
    content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
  <meta 
    http-equiv="X-UA-Compatible" 
    content="ie=edge">
  <meta 
    name="theme-color" 
    content="#fff" 
    id="theme-color">
  <meta 
    name="description" 
    content="Hexo">
  <link 
    rel="icon" 
    href="http://imgos.cn/2024/07/27/66a49edd1a14f.jpg">
  <title>python爬虫系列:beautifulsoup爬虫与线程池</title>
  
    
      <meta 
        property="og:title" 
        content="python爬虫系列:beautifulsoup爬虫与线程池">
    
    
      <meta 
        property="og:url" 
        content="https://ximenxiaoxue.github.io/2024/06/21/python%E7%88%AC%E8%99%AB%E7%B3%BB%E5%88%97-beautifulsoup%E7%88%AC%E8%99%AB%E4%B8%8E%E7%BA%BF%E7%A8%8B%E6%B1%A0/index.html">
    
    
      <meta 
        property="og:img" 
        content="http://imgos.cn/2024/07/27/66a49edd1a14f.jpg">
    
    
    
      <meta 
        property="og:type" 
        content="article">
      <meta 
        property="og:article:published_time" 
        content="2024-06-21">
      <meta 
        property="og:article:modified_time" 
        content="2024-07-27">
      <meta 
        property="og:article:author" 
        content="西门啸雪">
      
        
          <meta 
            property="og:article:tag" 
            content="爬虫">
        
      
    
  
  
  <link rel="preload" href="//at.alicdn.com/t/font_1946621_i1kgafibvw.css" as="style" >
  <link rel="preload" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css" as="style" >
  <link rel="preload" href="/css/main.css" as="style" >
  
  <link rel="modulepreload" href="//instant.page/5.1.0">
  
  <link rel="stylesheet" href="/css/main.css">
  
  <link rel="stylesheet" href="//at.alicdn.com/t/font_1946621_i1kgafibvw.css">
  
  <link rel="stylesheet" href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css">
  
  
  
    <link rel="stylesheet" href="/js/lib/lightbox/baguetteBox.min.css">
  
  <script>
    function loadScript(url, cb) {
      var script = document.createElement('script');
      script.src = url;
      if (cb) script.onload = cb;
      script.async = true;
      document.body.appendChild(script);
    }
    function loadCSS(href, data, attr) {
      var sheet = document.createElement('link');
      sheet.ref = 'stylesheet';
      sheet.href = href;
      sheet.dataset[data] = attr;
      document.head.appendChild(sheet);
    }
    function changeCSS(cssFile, data, attr) {
      var oldlink = document.querySelector(data);
      var newlink = document.createElement("link");
      newlink.setAttribute("rel", "stylesheet");
      newlink.setAttribute("href", cssFile);
      newlink.dataset.prism = attr;
      document.head.replaceChild(newlink, oldlink);
    }
  </script>
  
    
  
  <script>
    // control reverse button
    var reverseDarkList = {
      dark: 'light',
      light: 'dark'
    };
    var themeColor = {
      dark: '#1c1c1e',
      light: '#fff'
    }
    // get the data of css prefers-color-scheme
    var getCssMediaQuery = function() {
      return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
    };
    // reverse current darkmode setting function
    var reverseDarkModeSetting = function() {
      var setting = localStorage.getItem('user-color-scheme');
      if(reverseDarkList[setting]) {
        setting = reverseDarkList[setting];
      } else if(setting === null) {
        setting = reverseDarkList[getCssMediaQuery()];
      } else {
        return;
      }
      localStorage.setItem('user-color-scheme', setting);
      return setting;
    };
    // apply current darkmode setting
  </script>
  
    <script>
      var setDarkmode = function(mode) {
      var setting = mode || localStorage.getItem('user-color-scheme');
      if(setting === getCssMediaQuery()) {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[setting];
        document.getElementById('theme-color').dataset.mode = setting;
      } else if(reverseDarkList[setting]) {
        document.documentElement.setAttribute('data-user-color-scheme', setting);
        document.getElementById('theme-color').content = themeColor[setting];
        document.getElementById('theme-color').dataset.mode = setting;
      } else {
        document.documentElement.removeAttribute('data-user-color-scheme');
        localStorage.removeItem('user-color-scheme');
        document.getElementById('theme-color').content = themeColor[getCssMediaQuery()];
        document.getElementById('theme-color').dataset.mode = getCssMediaQuery();
      }
    };
    setDarkmode();
    </script>
  
  
  
    <link rel="preload" href="/js/lib/lightbox/baguetteBox.min.js" as="script">
    <link rel="preload" href="/js/lib/lightbox/baguetteBox.min.css" as="style" >
  
  
    <link rel="preload" href="/js/lib/lozad.min.js" as="script">
  
  
  
  
  
    
    <link rel="prefetch" href="//unpkg.com/valine/dist/Valine.min.js" as="script">
  
  
  <script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script>
  <script>LA.init({id:"3JKUPn8RBXzO8Bzv",ck:"3JKUPn8RBXzO8Bzv"})</script>
<meta name="generator" content="Hexo 7.3.0"></head>

  <body>
    <div class="wrapper">
       
      <nav class="navbar">
  <div class="navbar-logo">
    <a class="navbar-logo-main" href="/">
      
        <img 
          class="navbar-logo-img"
          width="32"
          height="32"
          src="http://imgos.cn/2024/07/27/66a49edd1a14f.jpg" 
          alt="blog logo">
      
      <span class="navbar-logo-dsc">AuroBreeze</span>
      </a>
  </div>
  <div class="navbar-menu">
    
      <a 
        href="/" 
        class="navbar-menu-item">
        
          首页
        
      </a>
    
      <a 
        href="/archives" 
        class="navbar-menu-item">
        
          归档
        
      </a>
    
      <a 
        href="/tags" 
        class="navbar-menu-item">
        
          标签
        
      </a>
    
      <a 
        href="/categories" 
        class="navbar-menu-item">
        
          分类
        
      </a>
    
      <a 
        href="/links" 
        class="navbar-menu-item">
        
          友链
        
      </a>
    
    <button 
      class="navbar-menu-item darknavbar navbar-menu-btn" 
      aria-label="Toggle dark mode"
      id="dark">
      <i class="iconfont icon-weather"></i>
    </button>
    <button 
      class="navbar-menu-item searchnavbar navbar-menu-btn" 
      aria-label="Toggle search"
      id="search">
      <!-- <i 
        class="iconfont icon-search" 
        style="font-size: 1.2rem; font-weight: 400;">
      </i> -->
      <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img"
        class="iconify iconify--ion" width="28" height="28" preserveAspectRatio="xMidYMid meet" viewBox="0 0 512 512">
        <path fill="none" stroke="currentColor" stroke-miterlimit="10" stroke-width="28"
          d="M256 80a176 176 0 1 0 176 176A176 176 0 0 0 256 80Z"></path>
        <path fill="none" stroke="currentColor" stroke-miterlimit="10" stroke-width="28"
          d="M232 160a72 72 0 1 0 72 72a72 72 0 0 0-72-72Z"></path>
        <path fill="none" stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="28"
          d="M283.64 283.64L336 336"></path>
      </svg>
    </button>
  </div>
</nav> 
      
      <div 
        id="local-search" 
        style="display: none">
        <input
          class="navbar-menu-item"
          id="search-input"
          placeholder="请输入搜索内容..." />
        <div id="search-content"></div>
      </div>
      
      <div class="section-wrap">
        <div class="container">
          <div class="columns">
            <aside class="left-column">
              
              <div class="card card-author">
                
  <img 
    src="http://imgos.cn/2024/07/27/66a49edd1a14f.jpg" 
    class="author-img"
    width="88"
    height="88"
    alt="author avatar">

<p class="author-name">西门啸雪</p>
<p class="author-description">我既从迷雾中走出，便留一地脚印。这是我的爱发电主页，非常感谢支持：https://afdian.net/a/AuroraBreeze</p>
<div class="author-message">
  <a 
    class="author-posts-count" 
    href="/archives">
    <span>6</span>
    <span>文章</span>
  </a>
  <a 
    class="author-categories-count" 
    href="/categories">
    <span>3</span>
    <span>分类</span>
  </a>
  <a 
    class="author-tags-count" 
    href="/tags">
    <span>4</span>
    <span>标签</span>
  </a>
</div>

  <div class="author-card-society">
    
      <div class="author-card-society-icon">
        <a target="_blank" rel="noopener" href="https://www.bilibili.com/">
          <i class="iconfont icon-bilibili society-icon"></i>
        </a>
      </div>
    
  </div>

              </div>
               <div class="sticky-tablet">
  
  
    <article class="display-when-two-columns spacer">
      <div class="card card-content toc-card">
        <div class="toc-header">
  <i 
    class="iconfont icon-menu" 
    style="padding-right: 2px;">
  </i>目录
</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#python%E7%88%AC%E8%99%AB%E7%B3%BB%E5%88%97-beautifulsoup%E7%88%AC%E8%99%AB%E4%B8%8E%E7%BA%BF%E7%A8%8B%E6%B1%A0"><span class="toc-text">python爬虫系列:beautifulsoup爬虫与线程池</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-text">前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Python%E4%BB%A3%E7%A0%81"><span class="toc-text">Python代码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E6%96%87"><span class="toc-text">正文</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C"><span class="toc-text">准备工作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81-%E5%88%86%E6%9E%90"><span class="toc-text">代码:分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81-%E8%8E%B7%E5%8F%96%E6%9C%80%E5%A4%A7%E9%A1%B5%E7%A0%81"><span class="toc-text">代码:获取最大页码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81-%E4%B8%8B%E8%BD%BD"><span class="toc-text">代码:下载</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81-%E4%BC%98%E5%8C%96%E5%AE%9E%E7%8E%B0"><span class="toc-text">代码:优化实现</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A6%82%E6%9E%9C%E4%BD%A0%E6%84%9F%E8%A7%89%E8%BF%99%E7%AF%87%E6%96%87%E7%AB%A0%E4%B8%8D%E9%94%99%EF%BC%8C%E8%80%8C%E4%B8%94%E8%B4%A2%E5%8A%9B%E5%85%85%E8%B6%B3%E7%9A%84%E8%AF%9D%EF%BC%8C%E5%B8%8C%E6%9C%9B%E5%8F%AF%E4%BB%A5%E7%94%A8%E7%88%B1%E5%8F%91%E7%94%B5%E6%94%AF%E6%8C%81%E4%B8%80%E4%B8%8B%E4%BD%9C%E8%80%85%EF%BC%8C%E6%84%9F%E8%B0%A2%E6%94%AF%E6%8C%81%EF%BC%81"><span class="toc-text">如果你感觉这篇文章不错，而且财力充足的话，希望可以用爱发电支持一下作者，感谢支持！</span></a></li></ol>
      </div>
    </article>
  
  
  <article class="card card-content categories-widget">
    <div class="categories-card">
  <div class="categories-header">
    <i 
      class="iconfont icon-fenlei" 
      style="padding-right: 2px;">
    </i>分类
  </div>
  <div class="categories-list">
    
      <a href="/categories/hexo/">
        <div class="categories-list-item">
          hexo
          <span class="categories-list-item-badge">2</span>
        </div>
      </a>
    
      <a href="/categories/python/">
        <div class="categories-list-item">
          python
          <span class="categories-list-item-badge">3</span>
        </div>
      </a>
    
      <a href="/categories/%E7%AE%97%E6%B3%95/">
        <div class="categories-list-item">
          算法
          <span class="categories-list-item-badge">1</span>
        </div>
      </a>
    
  </div>
</div>
  </article>
  
  <article class="card card-content tags-widget">
    <div class="tags-card">
  <div class="tags-header">
    <i 
      class="iconfont icon-biaoqian" 
      style="padding-right: 2px;">
    </i>热门标签
  </div>
  <div class="tags-list">
    
      <a 
        href="/tags/%E7%88%AC%E8%99%AB/" 
        title="爬虫">
        <div class="tags-list-item">爬虫</div>
      </a>
    
      <a 
        href="/tags/%E7%AE%97%E6%B3%95/" 
        title="算法">
        <div class="tags-list-item">算法</div>
      </a>
    
      <a 
        href="/tags/kaze/" 
        title="kaze">
        <div class="tags-list-item">kaze</div>
      </a>
    
      <a 
        href="/tags/hexo/" 
        title="hexo">
        <div class="tags-list-item">hexo</div>
      </a>
    
  </div>
</div>
  </article>
  
  
</div>
            </aside>
            <main class="main-column">
              
<article class="card card-content">
  <header>
    <h1 class="post-title">
      python爬虫系列:beautifulsoup爬虫与线程池
    </h1>
  </header>
  <div class="post-meta post-show-meta">
    <time datetime="2024-06-21T08:39:53.000Z">
      <i 
        class="iconfont icon-calendar" 
        style="margin-right: 2px;">
      </i>
      <span>2024-06-21</span>
    </time>
    
      <span class="dot"></span>
      
        <a 
          href="/categories/python/" 
          class="post-meta-link">
          python
        </a>
      
    
    
      <span class="dot"></span>
      <span>2.7k 字</span>
    
  </div>
  
    <div 
      class="post-meta post-show-meta" 
      style="margin-top: -10px;">
      <div style="display: flex; align-items: center;">
        <i 
          class="iconfont icon-biaoqian" 
          style="margin-right: 2px; font-size: 1.15rem;">
        </i>
        
          
          <a 
            href="/tags/%E7%88%AC%E8%99%AB/" 
            class="post-meta-link">
            爬虫
          </a>
        
      </div>
    </div>
  
  </header>
  <div 
    id="section" 
    class="post-content">
    <h1 id="python爬虫系列-beautifulsoup爬虫与线程池"><a href="#python爬虫系列-beautifulsoup爬虫与线程池" class="headerlink" title="python爬虫系列:beautifulsoup爬虫与线程池"></a>python爬虫系列:beautifulsoup爬虫与线程池</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这篇文章，也是对CSDN文章的修改，使用beautifulsoup爬取网页，并使用线程池提高爬取速度。</p>
<h2 id="Python代码"><a href="#Python代码" class="headerlink" title="Python代码"></a>Python代码</h2><pre class="highlight"><code class="hljs python"><br><span class="hljs-keyword">import</span> re<br><span class="hljs-keyword">from</span> concurrent.futures <span class="hljs-keyword">import</span> ThreadPoolExecutor <span class="hljs-comment">#导入线程池，加速爬取</span><br><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">from</span> bs4 <span class="hljs-keyword">import</span> BeautifulSoup<br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(<span class="hljs-string">&quot;webspider_book&quot;</span>):<br>    os.mkdir(<span class="hljs-string">&quot;webspider_book&quot;</span>)<br><br>url =<span class="hljs-string">&quot; https://www.bigee.cc/book/78647/&quot;</span><br><br>headers = &#123;<br>    <span class="hljs-string">&quot;User-Agent&quot;</span>: <span class="hljs-string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&quot;</span><br>&#125;<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_url</span>(<span class="hljs-params">url</span>):<span class="hljs-comment">#要用两次直接写个函数算了</span><br>    res = requests.get(url,headers=headers)<br>    soup = BeautifulSoup(res.text,<span class="hljs-string">&quot;lxml&quot;</span>)<br><br>    <span class="hljs-keyword">return</span> soup<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_max_page</span>(<span class="hljs-params">url</span>): <span class="hljs-comment">#获取下载的最大页数</span><br>    <span class="hljs-built_in">list</span> = get_url(url).select(<span class="hljs-string">&quot;body &gt; div.listmain &gt; dl &gt; dd a&quot;</span>)<br><br>    page_finall = re.findall(<span class="hljs-string">r&quot;/book/78647/(.*).html&quot;</span>, <span class="hljs-built_in">str</span>(<span class="hljs-built_in">list</span>[-<span class="hljs-number">1</span>][<span class="hljs-string">&#x27;href&#x27;</span>]))[<span class="hljs-number">0</span>] <span class="hljs-comment">#正则匹配最后一个章节的页码</span><br>    <span class="hljs-comment">#print(page_finall)</span><br><br>    <span class="hljs-keyword">return</span> page_finall <span class="hljs-comment">#返回最大页数</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_book_content</span>(<span class="hljs-params">url</span>):<span class="hljs-comment">#下载单个页面的函数</span><br><br>    soup = get_url(url)<br>    name = soup.select(<span class="hljs-string">&quot;#read &gt; div.book.reader &gt; div.content &gt; h1&quot;</span>)[<span class="hljs-number">0</span>].get_text()<br>    <span class="hljs-comment">#print(name)</span><br>    content = soup.select(<span class="hljs-string">&quot;#chaptercontent&quot;</span>) <span class="hljs-comment">#获取章节内容</span><br>    content = <span class="hljs-built_in">str</span>(content[<span class="hljs-number">0</span>].get_text()) <span class="hljs-comment">#获取章节内容</span><br>    <span class="hljs-comment">#print(content)</span><br><br>    f = <span class="hljs-built_in">open</span>(<span class="hljs-string">f&quot;webspider_book/<span class="hljs-subst">&#123;name&#125;</span>.txt&quot;</span>, <span class="hljs-string">&quot;w&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>)<br>    f.write(content)<span class="hljs-comment">#写入文件</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;name&#125;</span>下载完成----------&quot;</span>)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    num = <span class="hljs-built_in">int</span>(get_max_page(url=url))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;共<span class="hljs-subst">&#123;num&#125;</span>页----------&quot;</span>)<br>    <span class="hljs-keyword">with</span> ThreadPoolExecutor(<span class="hljs-number">100</span>) <span class="hljs-keyword">as</span> t: <span class="hljs-comment">#线程池大小为100</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,num+<span class="hljs-number">1</span>):<br>            t.submit(get_book_content,<span class="hljs-string">f&quot;https://www.bigee.cc/book/78647/<span class="hljs-subst">&#123;i&#125;</span>.html&quot;</span>) <span class="hljs-comment">#提交任务</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;线程池执行完毕&quot;</span>)<br></code></pre>

<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><hr>
<p>我们需要下载的是<code>requests</code>和<code>beautifulsoup4</code>库，如果没有安装，请使用以下命令安装,<code>lxml</code>也需要下载。</p>
<pre class="highlight"><code class="hljs python">pip install requests<br></code></pre>

<pre class="highlight"><code class="hljs python">pip install beautifulsoup4<br></code></pre>

<pre class="highlight"><code class="hljs python">pip install lxml<br></code></pre>

<hr>
<p>对于线程池，我们需要导入<code>concurrent.futures</code>库，并创建一个<code>ThreadPoolExecutor</code>对象，这个就不需要下载了，这是自带的，对于<code>concurren.futures</code>的使用，我们这里只讲解一下<code>ThreadPoolExecutor</code>的使用，其他的内容大家自行探索。</p>
<hr>
<h3 id="代码-分析"><a href="#代码-分析" class="headerlink" title="代码:分析"></a>代码:分析</h3><p><strong>思路分析</strong></p>
<p>这次我们的目标是爬取目标网站的小说，下载下来并用线程池加速下载,目标网址为：</p>
<p><a target="_blank" rel="noopener" href="https://www.bigee.cc/book/78647/">https://www.bigee.cc/book/78647/</a></p>
<p>好了，想要爬去内容，我们需要先分析网站的结构，去看看我们要爬取的内容在哪里。</p>
<blockquote>
<p>按 <strong>F12</strong> 进入开发者工具</p>
</blockquote>
<p><img  srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="/../blog_img/py_bs4_2/Snipaste_2024-06-21_17-03-30.png" class="lozad post-image"src="/../blog_img/py_bs4_2/Snipaste_2024-06-21_17-03-30.png"></p>
<p><img  srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="/../blog_img/py_bs4_2/Snipaste_2024-06-21_17-10-46.png" class="lozad post-image"src="/../blog_img/py_bs4_2/Snipaste_2024-06-21_17-10-46.png"></p>
<p>这样我们就可以看到了小说的地址到底是不是，我们可以进去看一看。</p>
<p><img  srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="/../blog_img/py_bs4_2/Snipaste_2024-06-21_17-12-54.png" class="lozad post-image"src="/../blog_img/py_bs4_2/Snipaste_2024-06-21_17-12-54.png"></p>
<p>我们也知道，我们看到的是相对的位置，然后我们在小说内容的部分看到了他的地址</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://www.bigee.cc/book/78647/1.html">https://www.bigee.cc/book/78647/1.html</a></p>
</blockquote>
<p>我们看到的相对位置</p>
<blockquote>
<p>&#x2F;book&#x2F;78647&#x2F;1.html</p>
</blockquote>
<p>我们这样就察觉到他们之间不就只差了一个<code>https://www.bigee.cc</code>吗？到时候我们只需要拼接一下，就可以拿到我们想要的页面的url了。</p>
<p>我们再看其他章节的地址，我们可以看到，他们的地址都是类似的，只是最后的数字不同。</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://www.bigee.cc/book/78647/2.html">https://www.bigee.cc/book/78647/2.html</a></p>
</blockquote>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://www.bigee.cc/book/78647/3.html">https://www.bigee.cc/book/78647/3.html</a><br><img  srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="/../blog_img/py_bs4_2/Snipaste_2024-06-21_17-16-45.png" class="lozad post-image"src="/../blog_img/py_bs4_2/Snipaste_2024-06-21_17-16-45.png"></p>
</blockquote>
<p>所以如果我们想要下载所有的章节，我们只需要遍历所有的数字，然后拼接一下就可以了。</p>
<p>那我们就需要先获取到这个最大的页码，然后遍历所有的数字，再在页面中下载内容就可以了。</p>
<h3 id="代码-获取最大页码"><a href="#代码-获取最大页码" class="headerlink" title="代码:获取最大页码"></a>代码:获取最大页码</h3><p>那我们现在要做的就是获取最大的页码，我们可以用<code>requests</code>和<code>beautifulsoup4</code>来获取。</p>
<pre class="highlight"><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">from</span> bs4 <span class="hljs-keyword">import</span> BeautifulSoup<br><span class="hljs-keyword">import</span> re<br><br>url =<span class="hljs-string">&quot; https://www.bigee.cc/book/78647/&quot;</span><br><br>headers = &#123;<br>    <span class="hljs-string">&quot;User-Agent&quot;</span>: <span class="hljs-string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&quot;</span><br>&#125; <span class="hljs-comment">#设置请求头,模拟浏览器</span><br><br>res = requests.get(url,headers=headers).text<span class="hljs-comment">#获取网页内容</span><br><br>soup = BeautifulSoup(res,<span class="hljs-string">&quot;lxml&quot;</span>)<span class="hljs-comment">#解析网页内容,加载lxml解析器</span><br><br><span class="hljs-built_in">list</span> = soup.select(<span class="hljs-string">&quot;body &gt; div.listmain &gt; dl &gt; dd a&quot;</span>)<br><br>page_finall = re.findall(<span class="hljs-string">r&quot;/book/78647/(.*).html&quot;</span>,<span class="hljs-built_in">list</span>[-<span class="hljs-number">1</span>][<span class="hljs-string">&#x27;href&#x27;</span>])<br><span class="hljs-built_in">print</span>(page_finall)<br></code></pre>

<p>这里我们使用<code>beautifulsoup</code>的<code>select</code>来获取到所有的章节的链接，也就是拿到所以我们匹配的<code>a</code>标签，这会生成一个列表。</p>
<p>对于BS4怎么去选择这个标签，我们也可以直接去浏览器进行复制，具体BS4的选择方法，大家可以自行探索。</p>
<p>下面是复制的步骤</p>
<p><img  srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="/../blog_img/py_bs4_2/Snipaste_2024-06-21_17-29-22.png" class="lozad post-image"src="/../blog_img/py_bs4_2/Snipaste_2024-06-21_17-29-22.png"></p>
<p><img  srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="/../blog_img/py_bs4_2/Snipaste_2024-06-21_17-29-57.png" class="lozad post-image"src="/../blog_img/py_bs4_2/Snipaste_2024-06-21_17-29-57.png"></p>
<p>但是我们复制的结果是：<code>body &gt; div.listmain &gt; dl &gt; dd:nth-child(2) &gt; a</code></p>
<p>因为我们要拿到最后一个章节的链接，而这个选择只能让我们拿到第一个章节的url，所以我们需要改一下，改为：<code>body &gt; div.listmain &gt; dl &gt; dd a</code></p>
<p>我们就能拿到这样的信息了。</p>
<p><img  srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="/../blog_img/py_bs4_2/Snipaste_2024-06-21_17-33-03.png" class="lozad post-image"src="/../blog_img/py_bs4_2/Snipaste_2024-06-21_17-33-03.png"></p>
<p>这是一个列表，最后的章节在列表的最后一个，我们可以用<code>list[-1]</code>来获取到最后一个章节的链接。</p>
<p>我们通过<code>list[-1][&#39;href&#39;]</code>来获取到最后一个章节的链接,这是bs4的一种方法。</p>
<p>接下来我们用正则表达式来匹配这个链接，正则表达式的匹配规则是：<code>r&quot;/book/78647/(.*).html&quot;</code></p>
<blockquote>
<p>其中<code>（.*）</code>的意思是匹配任意字符，当然必须在<code>/book/78647/</code>和<code>.html</code>之间。</p>
</blockquote>
<p>对于正则的使用可以来这里学习：<a target="_blank" rel="noopener" href="https://deerchao.cn/tutorials/regex/regex.htm">https://deerchao.cn/tutorials/regex/regex.htm</a></p>
<p>这样最后我们就拿到了最大的页码了，也就是313</p>
<p><img  srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="/../blog_img/py_bs4_2/Snipaste_2024-06-21_17-43-03.png" class="lozad post-image"src="/../blog_img/py_bs4_2/Snipaste_2024-06-21_17-43-03.png"></p>
<hr>
<blockquote>
<p><strong>在这个网站中还有点小问题他隐藏了一部分章节的url在另一个代码块中，大家可以去看看</strong></p>
</blockquote>
<hr>
<h3 id="代码-下载"><a href="#代码-下载" class="headerlink" title="代码:下载"></a>代码:下载</h3><p>我们现在需要先写一个代码把第一页的内容都下载下来，其他的页面，除了url不一样以外其他的相同。</p>
<p>我们先进入第一页，F12进入开发者工具，分析一下。</p>
<p><img  srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="/../blog_img/py_bs4_2/Snipaste_2024-06-21_17-47-29.png" class="lozad post-image"src="/../blog_img/py_bs4_2/Snipaste_2024-06-21_17-47-29.png"></p>
<pre class="highlight"><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">from</span> bs4 <span class="hljs-keyword">import</span> BeautifulSoup<br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(<span class="hljs-string">&quot;book&quot;</span>):<br>    os.mkdir(<span class="hljs-string">&quot;book&quot;</span>)<br><br>headers = &#123;<br>    <span class="hljs-string">&quot;User-Agent&quot;</span>: <span class="hljs-string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&quot;</span><br>&#125;<br><br><br><br>url = <span class="hljs-string">&quot;https://www.bigee.cc/book/78647/1.html&quot;</span><br>res = requests.get(url,headers=headers).text<br>soup = BeautifulSoup(res,<span class="hljs-string">&quot;lxml&quot;</span>)<br><br>name = soup.select(<span class="hljs-string">&quot;#read &gt; div.book.reader &gt; div.content &gt; h1&quot;</span>)[<span class="hljs-number">0</span>].get_text() <span class="hljs-comment">#获取小说名</span><br><span class="hljs-comment">#print(name)</span><br><br>content = soup.select(<span class="hljs-string">&quot;#chaptercontent&quot;</span>)<span class="hljs-comment">#获取章节内容</span><br>content = <span class="hljs-built_in">str</span>(content[<span class="hljs-number">0</span>].get_text())<span class="hljs-comment">#获取章节内容</span><br><span class="hljs-built_in">print</span>(content)<br><br><br>f = <span class="hljs-built_in">open</span>(<span class="hljs-string">f&quot;webspider_book/<span class="hljs-subst">&#123;name&#125;</span>.txt&quot;</span>,<span class="hljs-string">&quot;w&quot;</span>,encoding=<span class="hljs-string">&quot;utf-8&quot;</span>)<br>f.write(content)<br><br></code></pre>

<p><code>bs4</code>中获取标签内容的方法，是使用<code>get_text()</code> 方法，我们可以直接获取到章节的内容，但是我们用的<code>select</code>方法，返回的是一个列表，我们就需要拿到列表中的内容，当然，列表中也就只有这一个元素。因为我们选择器选择的<code>#chaptercontent</code>标签（浏览器复制的地址就是这个），所以我们直接用<code>content[0]</code>就能拿到章节的标签，然后用<code>get_text()</code>方法来获取到章节的内容。</p>
<p><img  srcset="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-src="/../blog_img/py_bs4_2/Snipaste_2024-06-21_18-04-12.png" class="lozad post-image"src="/../blog_img/py_bs4_2/Snipaste_2024-06-21_18-04-12.png"></p>
<p>小说名同理，直接去网页上用浏览器直接复制地址就可以了。</p>
<p>然后我们把章节的内容写入文件，文件名就是小说名，我们用<code>f = open(f&quot;webspider_book/&#123;name&#125;.txt&quot;,&quot;w&quot;,encoding=&quot;utf-8&quot;)</code>来创建文件，然后用<code>f.write(content)</code>来写入内容。</p>
<p>这样我们就下载了第一页的内容。</p>
<hr>
<h3 id="代码-优化实现"><a href="#代码-优化实现" class="headerlink" title="代码:优化实现"></a>代码:优化实现</h3><p>我们现在已经有了下载单个页面的函数，我们可以用线程池来加速下载，我们可以用<code>ThreadPoolExecutor</code>来创建线程池，然后用<code>submit()</code>方法来提交任务。</p>
<p>简单来说，我们不需要用循环来慢慢下载网页内容了，我们直接用线程池来下载，线程池创建本来就需要循环，我们把有规律的url遍历提交给线程池。</p>
<p>正常放前面不经常用和经常用但不需要改动的数据</p>
<pre class="highlight"><code class="hljs python"><br><span class="hljs-keyword">import</span> re<br><span class="hljs-keyword">from</span> concurrent.futures <span class="hljs-keyword">import</span> ThreadPoolExecutor <span class="hljs-comment">#导入线程池，加速爬取</span><br><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">from</span> bs4 <span class="hljs-keyword">import</span> BeautifulSoup<br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(<span class="hljs-string">&quot;webspider_book&quot;</span>):<br>    os.mkdir(<span class="hljs-string">&quot;webspider_book&quot;</span>)<br><br>url =<span class="hljs-string">&quot; https://www.bigee.cc/book/78647/&quot;</span><br><br>headers = &#123;<br>    <span class="hljs-string">&quot;User-Agent&quot;</span>: <span class="hljs-string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&quot;</span><br>&#125;<br><br></code></pre>

<p>在前面的讲述中</p>
<pre class="highlight"><code class="hljs python">res = requests.get(url,headers=headers)<br>soup = BeautifulSoup(res.text,<span class="hljs-string">&quot;lxml&quot;</span>)<br></code></pre>

<p>用到了两次，所以就干脆写了个函数，这样就不用重复写了，让代码看起来干净些。</p>
<pre class="highlight"><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_url</span>(<span class="hljs-params">url</span>):<span class="hljs-comment">#要用两次直接写个函数算了</span><br>    res = requests.get(url,headers=headers)<br>    soup = BeautifulSoup(res.text,<span class="hljs-string">&quot;lxml&quot;</span>)<br><br>    <span class="hljs-keyword">return</span> soup<br></code></pre>

<p>因为我们需要实例化，所以这个函数就得把实例化的对象<code>soup</code>返回，这样才可以使用。</p>
<pre class="highlight"><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_max_page</span>(<span class="hljs-params">url</span>): <span class="hljs-comment">#获取下载的最大页数</span><br>    <span class="hljs-built_in">list</span> = get_url(url).select(<span class="hljs-string">&quot;body &gt; div.listmain &gt; dl &gt; dd a&quot;</span>)<br><br>    page_finall = re.findall(<span class="hljs-string">r&quot;/book/78647/(.*).html&quot;</span>, <span class="hljs-built_in">str</span>(<span class="hljs-built_in">list</span>[-<span class="hljs-number">1</span>][<span class="hljs-string">&#x27;href&#x27;</span>]))[<span class="hljs-number">0</span>] <span class="hljs-comment">#正则匹配最后一个章节的页码</span><br>    <span class="hljs-comment">#print(page_finall)</span><br><br>    <span class="hljs-keyword">return</span> page_finall <span class="hljs-comment">#返回最大页数</span><br><br></code></pre>

<p>这个函数就是获取最大的页码,但是他也需要先实例化<code>soup</code>对象，所以我们就把<code>get_url()</code>函数也写进来，让他实例化并返回<code>soup</code>对象，最后正则获取到最后一个章节的url。</p>
<pre class="highlight"><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_book_content</span>(<span class="hljs-params">url</span>):<span class="hljs-comment">#下载单个页面的函数</span><br><br>    soup = get_url(url)<br>    name = soup.select(<span class="hljs-string">&quot;#read &gt; div.book.reader &gt; div.content &gt; h1&quot;</span>)[<span class="hljs-number">0</span>].get_text()<br>    <span class="hljs-comment">#print(name)</span><br>    content = soup.select(<span class="hljs-string">&quot;#chaptercontent&quot;</span>) <span class="hljs-comment">#获取章节内容</span><br>    content = <span class="hljs-built_in">str</span>(content[<span class="hljs-number">0</span>].get_text()) <span class="hljs-comment">#获取章节内容</span><br>    <span class="hljs-comment">#print(content)</span><br><br>    f = <span class="hljs-built_in">open</span>(<span class="hljs-string">f&quot;webspider_book/<span class="hljs-subst">&#123;name&#125;</span>.txt&quot;</span>, <span class="hljs-string">&quot;w&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>)<br>    f.write(content)<span class="hljs-comment">#写入文件</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;name&#125;</span>下载完成----------&quot;</span>)<br><br></code></pre>

<p>这个函数是下载对应页面的内容，他也需要先实例化<code>soup</code>对象，所以我们继续用我们刚开始写的<code>get_url()</code>函数来实例化，然后用<code>select()</code>方法来获取到小说名，然后用<code>get_text()</code>方法来获取到小说名和章节内容。</p>
<pre class="highlight"><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    num = <span class="hljs-built_in">int</span>(get_max_page(url=url))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;共<span class="hljs-subst">&#123;num&#125;</span>页----------&quot;</span>)<br>    <span class="hljs-keyword">with</span> ThreadPoolExecutor(<span class="hljs-number">100</span>) <span class="hljs-keyword">as</span> t: <span class="hljs-comment">#线程池大小为100</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,num+<span class="hljs-number">1</span>):<br>            t.submit(get_book_content,<span class="hljs-string">f&quot;https://www.bigee.cc/book/78647/<span class="hljs-subst">&#123;i&#125;</span>.html&quot;</span>) <span class="hljs-comment">#提交任务</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;线程池执行完毕&quot;</span>)<br></code></pre>

<p>在这里我们先运行<code>get_max_page()</code>函数来获取最大的页码，url我们在上面已经先声明了，然后我们用<code>ThreadPoolExecutor</code>来创建线程池，线程池大小为100 **(可以自己调整)**，然后用<code>for</code>循环来遍历所有的页码，然后用<code>submit()</code>方法来提交任务，任务就是<code>get_book_content()</code>函数，我们把页码和url拼接起来，然后提交给线程池,这样我们就能很快的把小说爬完了。</p>
<hr>
<h1 id="如果你感觉这篇文章不错，而且财力充足的话，希望可以用爱发电支持一下作者，感谢支持！"><a href="#如果你感觉这篇文章不错，而且财力充足的话，希望可以用爱发电支持一下作者，感谢支持！" class="headerlink" title="如果你感觉这篇文章不错，而且财力充足的话，希望可以用爱发电支持一下作者，感谢支持！"></a>如果你感觉这篇文章不错，而且财力充足的话，希望可以用爱发电支持一下作者，感谢支持！</h1><blockquote>
<p>爱发电地址:<a target="_blank" rel="noopener" href="https://afdian.com/a/AuroraBreeze">https://afdian.com/a/AuroraBreeze</a></p>
</blockquote>

  </div>
  <div>
    
      <div 
        class="post-note note-warning copyright" 
        style="margin-top: 42px">
        <p>
          <span style="font-weight: bold;">作者：</span><a 
            target="_blank" 
            rel="nofollow noopener noreferrer" 
            href="/about">
            西门啸雪
          </a>
        </p>
        <p>
          <span style="font-weight: bold;">文章链接：</span><a 
            target="_blank" 
            rel="nofollow noopener noreferrer" 
            href="https://ximenxiaoxue.github.io/2024/06/21/python%E7%88%AC%E8%99%AB%E7%B3%BB%E5%88%97-beautifulsoup%E7%88%AC%E8%99%AB%E4%B8%8E%E7%BA%BF%E7%A8%8B%E6%B1%A0/">
            https://ximenxiaoxue.github.io/2024/06/21/python%E7%88%AC%E8%99%AB%E7%B3%BB%E5%88%97-beautifulsoup%E7%88%AC%E8%99%AB%E4%B8%8E%E7%BA%BF%E7%A8%8B%E6%B1%A0/
          </a>
        </p>
        <p><span style="font-weight: bold;">版权声明：</span>本博客所有文章除特别声明外，均采用<a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0 协议</a>。转载请注明出处！</p>
      </div>
    
  </div>
</article>
<div class="nav">
  
    <div class="nav-item-prev">
      <a 
        href="/2024/07/09/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/" 
        class="nav-link">
        <i class="iconfont icon-left nav-prev-icon"></i>
        <div>
          <div class="nav-label">上一篇</div>
          
            <div class="nav-title">排序算法 </div>
          
        </div>
      </a>
    </div>
  
  
    <div class="nav-item-next">
      <a 
        href="/2024/06/21/kaze%E9%85%8D%E7%BD%AE%E5%BB%BA%E8%AE%AE/" 
        class="nav-link">
        <div>
          <div class="nav-label">下一篇</div>
          
            <div class="nav-title">kaze主题配置建议 </div>
          
        </div>
        <i class="iconfont icon-right nav-next-icon"></i>
      </a>
    </div>
  
</div>

  <div 
    class="card card-content comment-card" 
    style="margin-top: 16px;">
    <div class="comment-card-title">评论</div>
    
  <div id="vcomments"></div>
  
  <script>
    loadScript("//unpkg.com/valine/dist/Valine.min.js");
    var oldLoadVa = window.onload;
    window.onload = function () {
      oldLoadVa && oldLoadVa();
      new Valine({
        el: '#vcomments',
        appId: 'tRyzSH5lHX0I5brThoqaOrxx-gzGzoHsz',
        appKey: 'ZQpppITbmXCyEPCotughfXEJ',
        placeholder: 'Just go go',
        path: window.location.pathname,
        avatar: 'mp',
        meta: ["nick","mail","link"],
        pageSize: '10',
        lang: '',
        visitor: 'true',
        highlight: true,
        recordIP: true,
        
        
        
        enableQQ: 'true',
        requiredFields: ["nick","mail"],
      });
    };
  </script>

  </div>

<div 
  class="card card-content toc-card" 
  id="mobiletoc">
  <div class="toc-header">
  <i 
    class="iconfont icon-menu" 
    style="padding-right: 2px;">
  </i>目录
</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#python%E7%88%AC%E8%99%AB%E7%B3%BB%E5%88%97-beautifulsoup%E7%88%AC%E8%99%AB%E4%B8%8E%E7%BA%BF%E7%A8%8B%E6%B1%A0"><span class="toc-text">python爬虫系列:beautifulsoup爬虫与线程池</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-text">前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Python%E4%BB%A3%E7%A0%81"><span class="toc-text">Python代码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E6%96%87"><span class="toc-text">正文</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C"><span class="toc-text">准备工作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81-%E5%88%86%E6%9E%90"><span class="toc-text">代码:分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81-%E8%8E%B7%E5%8F%96%E6%9C%80%E5%A4%A7%E9%A1%B5%E7%A0%81"><span class="toc-text">代码:获取最大页码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81-%E4%B8%8B%E8%BD%BD"><span class="toc-text">代码:下载</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81-%E4%BC%98%E5%8C%96%E5%AE%9E%E7%8E%B0"><span class="toc-text">代码:优化实现</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A6%82%E6%9E%9C%E4%BD%A0%E6%84%9F%E8%A7%89%E8%BF%99%E7%AF%87%E6%96%87%E7%AB%A0%E4%B8%8D%E9%94%99%EF%BC%8C%E8%80%8C%E4%B8%94%E8%B4%A2%E5%8A%9B%E5%85%85%E8%B6%B3%E7%9A%84%E8%AF%9D%EF%BC%8C%E5%B8%8C%E6%9C%9B%E5%8F%AF%E4%BB%A5%E7%94%A8%E7%88%B1%E5%8F%91%E7%94%B5%E6%94%AF%E6%8C%81%E4%B8%80%E4%B8%8B%E4%BD%9C%E8%80%85%EF%BC%8C%E6%84%9F%E8%B0%A2%E6%94%AF%E6%8C%81%EF%BC%81"><span class="toc-text">如果你感觉这篇文章不错，而且财力充足的话，希望可以用爱发电支持一下作者，感谢支持！</span></a></li></ol>
</div>
            </main>
            <aside class="right-column">
              <div class="sticky-widescreen">
  
  
    <article class="card card-content toc-card">
      <div class="toc-header">
  <i 
    class="iconfont icon-menu" 
    style="padding-right: 2px;">
  </i>目录
</div>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#python%E7%88%AC%E8%99%AB%E7%B3%BB%E5%88%97-beautifulsoup%E7%88%AC%E8%99%AB%E4%B8%8E%E7%BA%BF%E7%A8%8B%E6%B1%A0"><span class="toc-text">python爬虫系列:beautifulsoup爬虫与线程池</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-text">前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Python%E4%BB%A3%E7%A0%81"><span class="toc-text">Python代码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E6%96%87"><span class="toc-text">正文</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C"><span class="toc-text">准备工作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81-%E5%88%86%E6%9E%90"><span class="toc-text">代码:分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81-%E8%8E%B7%E5%8F%96%E6%9C%80%E5%A4%A7%E9%A1%B5%E7%A0%81"><span class="toc-text">代码:获取最大页码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81-%E4%B8%8B%E8%BD%BD"><span class="toc-text">代码:下载</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81-%E4%BC%98%E5%8C%96%E5%AE%9E%E7%8E%B0"><span class="toc-text">代码:优化实现</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A6%82%E6%9E%9C%E4%BD%A0%E6%84%9F%E8%A7%89%E8%BF%99%E7%AF%87%E6%96%87%E7%AB%A0%E4%B8%8D%E9%94%99%EF%BC%8C%E8%80%8C%E4%B8%94%E8%B4%A2%E5%8A%9B%E5%85%85%E8%B6%B3%E7%9A%84%E8%AF%9D%EF%BC%8C%E5%B8%8C%E6%9C%9B%E5%8F%AF%E4%BB%A5%E7%94%A8%E7%88%B1%E5%8F%91%E7%94%B5%E6%94%AF%E6%8C%81%E4%B8%80%E4%B8%8B%E4%BD%9C%E8%80%85%EF%BC%8C%E6%84%9F%E8%B0%A2%E6%94%AF%E6%8C%81%EF%BC%81"><span class="toc-text">如果你感觉这篇文章不错，而且财力充足的话，希望可以用爱发电支持一下作者，感谢支持！</span></a></li></ol>
    </article>
  
  
  <article class="card card-content">
    <div class="recent-posts-card">
  <div class="recent-posts-header">
    <i 
      class="iconfont icon-wenzhang_huaban" 
      style="padding-right: 2px;">
    </i>最近文章
  </div>
  <div class="recent-posts-list">
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2024-07-29</div>
        <a href="/2024/07/29/python%E7%88%AC%E8%99%AB%E7%B3%BB%E5%88%97-JS%E9%80%86%E5%90%91%E7%BD%91%E6%98%93%E4%BA%91%E8%AF%84%E8%AE%BA/"><div class="recent-posts-item-content">python爬虫系列-JS逆向网易云评论</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2024-07-09</div>
        <a href="/2024/07/09/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"><div class="recent-posts-item-content">排序算法</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2024-06-21</div>
        <a href="/2024/06/21/python%E7%88%AC%E8%99%AB%E7%B3%BB%E5%88%97-beautifulsoup%E7%88%AC%E8%99%AB%E4%B8%8E%E7%BA%BF%E7%A8%8B%E6%B1%A0/"><div class="recent-posts-item-content">python爬虫系列:beautifulsoup爬虫与线程池</div></a>
      </div>
    
      <div class="recent-posts-item">
        <div class="recent-posts-item-title">2024-06-21</div>
        <a href="/2024/06/21/kaze%E9%85%8D%E7%BD%AE%E5%BB%BA%E8%AE%AE/"><div class="recent-posts-item-content">kaze主题配置建议</div></a>
      </div>
    
  </div>
</div>
  </article>
  
  
</div>
            </aside>
          </div>
        </div>
      </div>
    </div>
     
    <footer class="footer">
  <div class="footer-container">
    <div>
      <div class="footer-dsc">
        <span>
          Copyright ©
          
            2020
          
          
                - 
                2024
          
        </span>
        &nbsp;
        <a 
          href="/" 
          class="footer-link">
          AuroBreeze
        </a>
      </div>
    </div>

    
      <div class="footer-dsc">
        
          Powered by
          <a 
            href="https://hexo.io/" 
            class="footer-link" 
            target="_blank" 
            rel="nofollow noopener noreferrer">
            &nbsp;Hexo
          </a>
        
        
          <span>&nbsp;|&nbsp;</span>
        
        
          Theme -
          <a 
            href="https://github.com/theme-kaze" 
            class="footer-link" 
            target="_blank"
            rel="nofollow noopener noreferrer">
            &nbsp;Kaze
          </a>
        
      </div>
    
    
    
    
</footer>
 
    
  <a 
    role="button" 
    id="scrollbutton" 
    class="basebutton" 
    aria-label="回到顶部">
    <i class="iconfont icon-arrowleft button-icon"></i>
  </a>

<a 
  role="button" 
  id="menubutton"
  aria-label="menu button"
  class="basebutton">
  <i class="iconfont icon-menu button-icon"></i>
</a>
<a 
  role="button" 
  id="popbutton" 
  class="basebutton" 
  aria-label="控制中心">
  <i class="iconfont icon-expand button-icon"></i>
</a>
<a 
  role="button" 
  id="darkbutton" 
  class="basebutton darkwidget" 
  aria-label="夜色模式">
  <i class="iconfont icon-weather button-icon"></i>
</a>
<a 
  role="button" 
  id="searchbutton" 
  class="basebutton searchwidget" 
  aria-label="搜索">
  <i class="iconfont icon-search button-icon"></i>
</a> 
     
     
     
      <script>
  var addImgLayout = function () {
    var img = document.querySelectorAll('.post-content img')
    var i
    for (i = 0; i < img.length; i++) {
      var wrapper = document.createElement('a')
      wrapper.setAttribute('href', img[i].getAttribute('data-src'))
      wrapper.setAttribute('aria-label', 'illustration')
      wrapper.style.cssText =
        'width: 100%; display: flex; justify-content: center;'
      if (img[i].alt) wrapper.dataset.caption = img[i].alt
      wrapper.dataset.nolink = true
      img[i].before(wrapper)
      wrapper.append(img[i])
      var divWrap = document.createElement('div')
      divWrap.classList.add('gallery')
      wrapper.before(divWrap)
      divWrap.append(wrapper)
    }
    baguetteBox.run('.gallery')
  }
</script>
<script>
  loadScript(
    "/js/lib/lightbox/baguetteBox.min.js",
    addImgLayout
  )
</script>
 
     
     
    <script src="/js/main.js"></script> 
     
    
      <script>
        var addLazyload = function () {
          var observer = lozad('.lozad', {
            load: function (el) {
              el.srcset = el.getAttribute('data-src')
            },
            loaded: function (el) {
              el.classList.add('loaded')
            },
          })
          observer.observe()
        }
      </script>
      <script>
        loadScript('/js/lib/lozad.min.js', addLazyload)
      </script>
    
    <script src="//instant.page/5.1.0" type="module"
      integrity="sha384-by67kQnR+pyfy8yWP4kPO12fHKRLHZPfEsiSXR8u2IKcTdxD805MGUXBzVPnkLHw"></script>
    
    
      <script>
        setTimeout(() => {localSearch("search.json")}, 0)
      </script>
    
  </body>
</html>
